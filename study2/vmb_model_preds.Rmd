---
title: "R Notebook"
output: 
  html_notebook: 
    code_folding: hide
    toc: true
    toc_float: true
---



```{r load in existing objects, message=FALSE, warning=FALSE}
# net.struct <- readRDS("net-struct-cogsci2018.rds")

source("../study1/vacc_import_data.R")
source("../scripts/gmod_tools.R")
source("../scripts/virtualEvidence.R")

library(bnlearn)
library(rjags)
library(patchwork)
```

```{r load and tidy study 3 data, include = F}
source("../scripts/load-data-s2.R")

stderr <- function(x) {
          sqrt(var(x[!is.na(x)]) / length(x[!is.na(x)]))
}


interv_df <- d_scored %>% 
              filter(condition=="diseaseRisk") %>%
              mutate(interv = ifelse(phase=="pre",0,1)) %>%
              group_by(workerId) %>%
              spread(scale, Mean)

obs_data <- d_scored %>% 
  filter(condition == "diseaseRisk") %>% 
  spread(phase, Mean) %>%
  mutate(score = post - pre) %>%
  group_by(scale) %>%
  summarize(
    observed = mean(score),
    std_error = stderr(score))
```

```{r}
# define some functions for doing and processing predictions

predict_change_scores <- function(model, data0, data1, nodes_to_predict, held_out_nodes=c(), iter=1e4) {
  # note: intervention must be named "interv"
  
  require(dplyr)
  
  preds_mean1 <- make_predictions(model, 
                                data1, 
                                nodes_to_predict, 
                                held_out_nodes,
                                iter = iter)

  preds_mean0 <- make_predictions(model, 
                                  data0, 
                                  nodes_to_predict, 
                                  held_out_nodes,
                                  iter = iter)
  
  preds_mean <- preds_mean1 - preds_mean0

}

process_predictions <- function(predictions, obs_data) {
  require(dplyr)
  predictions <- predictions %>% 
  select(-interv) %>%
  gather(scale, predicted) %>%
  left_join(obs_data, by = "scale") 
}

plot_processed_predictions <- function(processed_predictions) {
  require(dplyr)
  require(ggplot2)
  plt1 <- processed_predictions %>%
    ggplot(aes(x=predicted, y = observed)) +
    geom_point() +
    theme_bw() +
    theme(aspect.ratio = 1)

  plt2 <- processed_predictions %>% 
    # left_join(obs_error, by = "scale") %>%
    group_by(scale) %>%
    gather(type, change_score, observed, predicted) %>%
    mutate(std_error = ifelse(type == "observed", std_error, NA )) %>%
    mutate(
           UL = change_score + std_error*1.96,
           LL = change_score - std_error*1.96
           ) %>%
    ggplot(aes(y = reorder(scale, change_score),
               x = change_score,
               xmax = UL, 
               xmin = LL, 
               color = type)) +
    geom_point(shape=17, size=3) +
    geom_errorbarh(height=0) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  return(list(corr_plot = plt1, means_plot = plt2))
}

score_predictions <- function(processed_predictions) {
  list(
  r = cor(processed_predictions$observed, processed_predictions$predicted),
  r_squared = cor(processed_predictions$observed, processed_predictions$predicted)^2,
  mae = mean(abs(processed_predictions$observed - processed_predictions$predicted)),
  rmse = sqrt(mean((processed_predictions$observed - processed_predictions$predicted)^2))
  )
}

score_predictions_boot <- function(processed_predictions) {
  
  samps <- 1:1000
  
  preds <- lapply(samps, function(x){
    pp <-  processed_predictions %>% 
      mutate(resamp_observed = rnorm(14, observed, std_error))
    list(
    r = cor(pp$resamp_observed, pp$predicted),
    r_squared = cor(pp$resamp_observed, pp$predicted)^2,
    mae = mean(abs(pp$resamp_observed - pp$predicted)),
    rmse = sqrt(mean((pp$resamp_observed - pp$predicted)^2))
    )
  })
  
  
}

```


# Aggregate predictions

This notebook aims to examine/address the issue of predicting out-of-sample data using our model. In a previous notebook, `vacc_manyBelief_study3_modelsim.RMD`, I predicted average scale responses from study 3 using the previously learned model by conditioning on the Disease Severe node. 

In this notebook, I'll ... 

1. Explicitly model the intervention, rather than just conditioning disease severe.
2. Examine model as "causal model"
3. Explore model predictions under different assumptions about what beliefs are "targetted" by the intervention.
4. Similarly, explore whether we should model the intervention as providing direct evidence for multiple beliefs.
5. Consider naïve models ...


## cognitive model

First, let's examine the predictions of our cognitive model, where we treat the graphical model as a cognitive model, and reading the intervention materials as observing or conditioning on some evidence relevant to beliefs about disease severity.

To recap, we have ...

1. The cognitive network model (from study 1, n=1130 (data = `d_bn`))
2. A model of the relationship between disease severity and the intervention, modeled from study 2.

And we're predicting the average change scores observed in each condition following the disease risk manipulation in Study 2. So, for the most part this is an out-of-sample prediction task -- predicting change scores for new participants on the basis of a model generated from correlations observed among different participants, along with a bit of info about the intervention's effects on disease severity from the same participants.

```{r, message=FALSE, warning=FALSE}

nodes_to_predict <- bnlearn::nodes(net.struct)


cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn)

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

cog_model <- add_node(cog_model, glm_to_jags_eq(interv_fit))

# can also just use "model" here so long as you include held_out_nodes

preds_cog <- predict_change_scores(cog_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5
                                           )

# preds_cog$interv <- 0

preds_cog <- process_predictions(preds_cog, obs_data)

plots_cog <- plot_processed_predictions(preds_cog)

plots_cog$corr_plot
plots_cog$means_plot

score_predictions(preds_cog)

```

So it's a pretty good fit, as we expected. Note that the r = `r score_predictions(preds_cog)$r %>% round(3)` is higher than the correlation in `...modelsims.Rmd` notebook, because I'm here also including disease severity in calculating the fit.

## "Causal" model

Next we can rule out the (perhaps not-all-that tempting) argument that we haven't developed a cognitive model at all, instead perhaps a causal model describing how holding different beliefs _causes_ a person to hold other beliefs. verbally that might not sound all that different, but the difference comes in how we conceive of evidence affecting participants' beliefs. Instead of an edge directed from the belief to the evidence, it'd be an edge directed from the evidence to the belief. 

(Here I'm fudging things a bit in a way that should be fine considering these are linear models. I'm keeping the edge directed from disease severity to intervention, but then doing "graph surgery" to prune the disease severity node's parents when we predict the effects of the intervention. Since we're predicting changes, I'm about 99% confident this should work.)

```{r, message=FALSE, warning=FALSE}
library(bnlearn)
surgery_graph <- net.struct %>%
  drop.arc("vaccEff", "diseaseSevere") %>%
  drop.arc("infantImmLimCap", "diseaseSevere")

causal_arcs <- bnlearn_to_df(surgery_graph)

causal_model <- write_jags_model(causal_arcs, d_bn) %>% suppressWarnings()

interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

causal_model <- add_node(causal_model, glm_to_jags_eq(interv_fit)) # this is a cheat but it's ok ...

preds_causal <- predict_change_scores(causal_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5
                                           )

preds_causal <- process_predictions(preds_causal, obs_data)

plots_causal <- plot_processed_predictions(preds_causal)

plots_causal$corr_plot
plots_causal$means_plot

score_predictions(preds_causal)
```

Treating things as a causal model makes very poor predictions.

# What beliefs are targetted by the intervention?

Here we can pretend that expectations for the intervention materials are set by each of the 14 beliefs, and compare the accuracy of the resulting models' predictions.

```{r, message=FALSE, warning=FALSE}

all_beliefs <- nodes(net.struct)

cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

r_belief_targets <- lapply(all_beliefs, function(b){
  
  f <- paste0("interv ~ ", b)

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(as.formula(f), 
                    data = interv_df,
                    family = "binomial"
                    )
  
  cog_model <- add_node(cog_model, glm_to_jags_eq(interv_fit)) 
  
  # can also just use "model" here so long as you include held_out_nodes
  
  preds <- predict_change_scores(cog_model,
                                             data.frame(interv = 0),
                                           data.frame(interv = 1),
                                             nodes_to_predict,
                                             iter = 1e5
                                             )
  
  
  
  preds <- process_predictions(preds, obs_data)
  score_predictions(preds)
  }
)

names(r_belief_targets) <- all_beliefs

r_belief_targets %>% 
  data.frame() %>%
  gather(id, value) %>% 
  mutate(
    belief = gsub("\\.(.*)","",id), 
    metric = gsub("(.*)\\.","",id)
    ) %>%
  select(-id) %>%
  ggplot(aes(x = belief, y = value)) +
  geom_point() +
  facet_wrap(~ metric, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

By all metrics disease severity gives the best fit, with disease rarity in second place. Both seem highly plausible as targets of the intervention. None of the other beliefs come close.

# Playing with edge directions

The direction of three edges were set arbitrarily in our original cogsci model. Here I examine what happens if we change their directions (comparing the original model with the 6 other possible models).

```{r, message=FALSE, warning=FALSE}
# here I can swap things around
mynet <- net.struct

mynet1 <- reverse.arc(mynet, "hb","nat") # this might be the best
mynet2 <- reverse.arc(mynet, "vaccStrain","vaccDanger") 
mynet3 <- reverse.arc(mynet1, "vaccStrain","vaccDanger")
mynet4 <- reverse.arc(mynet2, "vaccTox","vaccDanger")
mynet5 <- reverse.arc(mynet3, "vaccTox","vaccDanger")

possible_dags <- list(
  mynet,
  mynet1,
  mynet2,
  mynet3,
  mynet4,
  mynet5
  )

r_dag_comparisons <- lapply(possible_dags, function(dag){
  
  dag_model <- write_jags_model(bnlearn_to_df(dag), d_bn) %>% suppressWarnings()

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(interv ~ diseaseSevere, 
                    data = interv_df,
                    family = "binomial"
                    )
  
  dag_model <- add_node(dag_model, glm_to_jags_eq(interv_fit)) 
  
  # can also just use "model" here so long as you include held_out_nodes
  
  preds <- predict_change_scores(dag_model,
                                             data.frame(interv = 0),
                                           data.frame(interv = 1),
                                             nodes_to_predict,
                                             iter = 1e5*2.5
                                             )
  
  
  
  preds <- process_predictions(preds, obs_data)
  score_predictions(preds)
  }
)

names(r_dag_comparisons) <- c(
  "original",
  "swap_hb_nat",
  "swap_vs_vd",
  "swap_hb_nat_vs_vd",
  "swap_vs_vd_vt_vd",
  "swap_hb_nat_vs_vd_vt_vd"
)

r_dag_comparisons %>% 
  data.frame() %>%
  gather(id, value) %>% 
  mutate(
    dag = gsub("\\.(.*)","",id), 
    metric = gsub("(.*)\\.","",id)
    ) %>%
  select(-id) %>%
  ggplot(aes(x = dag, y = value)) +
  geom_point() +
  facet_wrap(~ metric, scales = "free") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

This analysis doesn't hugely favor any of the models, and even seems sensitive to error in the predictions, so I've ramped mcmc sampels up to 250k.

# Modeling multiple targetted beliefs

The biggest outlier point for our cognitive model is the "parental protectiveness" (`overpar`) belief. Considering the evocative content of the intervention, it's possible that it directly bears on this belief. 

Here we can model our intervention targetting both disease severity and overparenting ...

```{r, message=FALSE, warning=FALSE}

cog_model2 <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere + overpar, 
                  data = interv_df,
                  family = "binomial"
                  )

cog_model2 <- add_node(cog_model2, glm_to_jags_eq(interv_fit))

# can also just use "model" here so long as you include held_out_nodes

preds_cog2 <- predict_change_scores(cog_model2,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5*2.5
                                           )



preds_cog2 <- process_predictions(preds_cog2, obs_data)

plots_cog2 <- plot_processed_predictions(preds_cog2)

plots_cog2$corr_plot
plots_cog2$means_plot

score_predictions(preds_cog2)
```

## Comparing models

This code is a bit hacky, should write some better bootstrapping code. But here is a quick comparison, with somewhat unclear results. Previously this looked more clear cut, to me suggesting there may be more volatility in the mcmc sampling than I had thought. I should maybe give them more burn-in.

```{r}
boot_scores_cog <- score_predictions_boot(preds_cog)
boot_scores_cog2 <- score_predictions_boot(preds_cog2)

tibble(cog = sapply(boot_scores_cog, function(x){x$rmse}), 
           cog2 = sapply(boot_scores_cog2, function(x){x$rmse})) %>%
  gather(model, rmse, cog, cog2) %>%
  ggplot(aes(x=model, y = rmse)) +
  geom_boxplot() +
  theme_bw()
```

# Naïve Models

Once we see that we have a cognitive model, I'll say the _cognitive model assumption_ is met. And when it's met, it actually has some interesting consequences. 

Because observations in a cognitive model (as in, exposure to educational materials) are conditioning events rather than causal interventions, we can actually do a good job predicting their effects using naïve models. What I mean by naïve models are models that naïvely assume all beliefs are connected to some central, focal belief(s)--specifically, whichever beliefs are targetted by educational interventions.

This works because all we need to predict the expected change is a model of the marginal conditional distribution of each variable. To put it in some math, our model approximates the joint distribution over all the variables $x_1$ through $x_n$:

$$P(x_1, x_2, x_3, ..., x_n)$$

But to ask how some other belief, $y$ will change on average, given some change in another belief $x$, that's something like this:

$$E[P(y|x=1) - P(y|x=0)]$$

And for that we only need to know the conditional probability, $P(y|x)$, which we can approximate in this naïve model. So here we have a model that naïvely assumes that disease severity beliefs influence all other beliefs.

```{r, message=FALSE, warning=FALSE}

vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere") %>% distinct()

# naïve causal model, diseaseSevere--> all (equivalent change score prediction)
nb_arcs <- data.frame(from = rep("diseaseSevere", 13), to = vars$scale)

nb_model <- write_jags_model(nb_arcs, d_bn) %>% suppressWarnings()


# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

nb_model <- add_node(nb_model, glm_to_jags_eq(interv_fit))


# can also just use "model" here so long as you include held_out_nodes

preds_nb <- predict_change_scores(nb_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5
                                           )

preds_nb <- process_predictions(preds_nb, obs_data)

plots_nb <- plot_processed_predictions(preds_nb)

plots_nb$corr_plot
plots_nb$means_plot

score_predictions(preds_nb)

```

The naïve model is comparable to the more complex cognitive model, at least when it comes to predicting the changes in beliefs following the interventions. Of course, as a model of the actual relationships among beliefs, it's terrible (see `...modelSims.Rmd` notebook for a brief exploration of that).

### Naïvely modeling multiple targetted beliefs

We can also model multiple direct evidence relations by extending the naïve model. There are probably multiple ways to extend it. What I've come up with here is to just assume that both disease severity and parental protectiveness are the two focal beliefs, and that they both influene all other beliefs. I suppose I could also add a connection between them one way or another.

```{r, message=FALSE, warning=FALSE}

vars2 <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere", scale!="overpar") %>% distinct()

# naïve causal model, diseaseSevere--> all (equivalent change score prediction)

nb_arcs2 <- data.frame(
  from = c(
    rep("diseaseSevere", 12),
    rep("overpar", 12)
  ),
  to = rep(vars2$scale, 2)
)

nb_model2 <- write_jags_model(nb_arcs2, d_bn) %>% suppressWarnings()

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere + overpar, 
                  data = interv_df,
                  family = "binomial"
                  )

nb_model2 <- add_node(nb_model2, glm_to_jags_eq(interv_fit))


# can also just use "model" here so long as you include held_out_nodes

preds_nb2 <- predict_change_scores(nb_model2,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5*2.5
                                           )

preds_nb2 <- process_predictions(preds_nb2, obs_data)

plots_nb2 <- plot_processed_predictions(preds_nb2)

plots_nb2$corr_plot
plots_nb2$means_plot

score_predictions(preds_nb2)

```

This model is the best of all!, getting a whopping `r score_predictions(preds_nb2)$r^2 %>% round(3) * 100`% of the variance! Not only that, RMSE and MAE also look somewhat lower than the other models. 

So, from a purely predictive standpoint, if the cognitive model assumption holds, then the naïve model might actually be the best approach to predicting belief changes. However I don't think there are any guarantees that this sort of thing will always work when thinking about multiple targetted beliefs, for one. Quickly comparing all four models, it really does look like the naïve model with both disease severity and overparenting targetted does best, for what it's worth.

```{r}
boot_scores_cog <- score_predictions_boot(preds_cog)
boot_scores_cog2 <- score_predictions_boot(preds_cog2)
boot_scores_nb <- score_predictions_boot(preds_nb)
boot_scores_nb2 <- score_predictions_boot(preds_nb2)

tibble(
  cog = sapply(boot_scores_cog, function(x){x$rmse}), 
  cog2 = sapply(boot_scores_cog2, function(x){x$rmse}),
  nb = sapply(boot_scores_nb, function(x){x$rmse}),
  nb2 = sapply(boot_scores_nb2, function(x){x$rmse}),
       ) %>%
  gather(model, rmse, cog, cog2, nb, nb2) %>%
  ggplot(aes(x=model, y = rmse)) +
  geom_boxplot() +
  theme_bw()
```

# Binning participants

One idea we had was to predict change scores for different bins of participants. The obvious binning variable to me is disease severity. Here I'm binning participants by disease severity quartile (4 bins), generating predictions within each bin, and plotting the performance individually. 

I used the same intervention node formula for each bin, so this is consistently the same model. So, all we're looking at here is whether the model's predictions hold for different groups of participants. In contrast, I could also refit the intervention node for each bin, but it's less clear what's being modeled in that case.

```{r, message=FALSE, warning=FALSE}
# let's try creating 4 bins of participants

cutpoints <- c(.25, .5, .75, 1)
q <- quantile(interv_df %>%
                        filter(phase=="pre") %>%
                        .$diseaseSevere, cutpoints)

interv_df_binned <- interv_df %>%
  ungroup() %>%
  mutate(workerId = as.character(workerId)) %>%
  mutate(bin = ifelse(diseaseSevere <= q[1], 1,
                      ifelse( diseaseSevere <= q[2], 2,
                              ifelse(diseaseSevere <= q[3],3,
                                     4))))
  
bin_ids <- list(
  interv_df_binned %>% filter(phase=="pre", bin==1) %>% .$workerId,
  interv_df_binned %>% filter(phase=="pre", bin==2) %>% .$workerId,
  interv_df_binned %>% filter(phase=="pre", bin==3) %>% .$workerId,
  interv_df_binned %>% filter(phase=="pre", bin==4) %>% .$workerId
)

data_bins <- lapply(bin_ids, function(x){
  interv_df %>%
    filter(workerId %in% x)
})

bin_preds <- lapply(data_bins, function(data){

  obs_data <- d_scored %>% 
    filter(condition == "diseaseRisk") %>% 
    mutate(workerId = as.character(workerId)) %>%
    filter(workerId %in% data$workerId) %>%
    spread(phase, Mean) %>%
    mutate(score = post - pre) %>%
    group_by(scale) %>%
    summarize(
      observed = mean(score),
      std_error = stderr(score)
      )
  
  model <- write_jags_model(bnlearn_to_df(net.struct), d_bn)
  # model <- write_jags_model(nb_arcs, d_bn) # produces similar predictions

  # # # can modify this to test assumptions about direct effects of intervention ...
  # interv_fit <- glm(interv ~ diseaseSevere + overpar,
  #                   data = data,
  #                   family = "binomial"
  #                   )
  
  model <- add_node(model, glm_to_jags_eq(interv_fit)) 
  
  # can also just use "model" here so long as you include held_out_nodes
  m0 <- data %>%
    filter(phase=="pre") %>%
    .$diseaseSevere %>%
    mean()
  
  m1 <- data %>%
    filter(phase=="post") %>%
    .$diseaseSevere %>%
    mean()
    
  preds <- predict_change_scores(model,
    data.frame(interv = 0),
    data.frame(interv = 1),
    # data.frame(diseaseSevere = m0),
    # data.frame(diseaseSevere = m1),
    nodes_to_predict,
    iter = 1e5
  ) %>% mutate(interv = 0)
  
  preds <- process_predictions(preds, obs_data)
  }
)

```

```{r}
# man this is hackey ...

all_bin_preds <- bin_preds[[1]] %>%
  mutate(bin = 1) %>%
  bind_rows(
    bin_preds[[2]] %>%
      mutate(bin = 2)
      ) %>%
  bind_rows(
    bin_preds[[3]] %>%
      mutate(bin = 3)
  ) %>%
  bind_rows(
    bin_preds[[4]] %>%
  mutate(bin = 4)
  )

all_bin_preds %>%
  # filter(scale!="diseaseSevere") %>%
  ggplot(aes(x=predicted, y = observed)) +
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~bin, scales="fixed", ncol = 2) +
  theme_bw() +
  coord_fixed(xlim = c(-.35,.8), ylim = c(-.35,.8)) +
  theme(aspect.ratio = 1) +
  labs(title = "Predictions by quartile bin (binned on disease severity)")
```

Correlations are strongly positive in the the first 3 quartiles, and less strong in the upper-most quartile. Of course that's where we have the most restriction of range. Though the correlations are similar, note the differences in the scales, with bigger changes observed in the 1st quartile especially. The model makes essentially (exactly?) the same predictions for all four bins, so it misses in different ways across the bins.

In case it helps, here it all is together in one plot ...

```{r}
all_bin_preds %>%
  # filter(scale!="diseaseSevere") %>%
  mutate(bin = as.factor(bin)) %>%
  ggplot(aes(x=predicted, y = observed, color = bin)) +
  geom_point() +
  # facet_wrap(~bin, scales="fixed", ncol = 2) +
  theme_bw() +
  coord_fixed(xlim = c(-.35,.8), ylim = c(-.35,.8)) +
  theme(aspect.ratio = 1) +
  labs(title = "Predictions by quartile bin\n(binned on disease severity)")
```

Overall, I think this is somewhat encouraging, but it's not quite as clear or revealing as I had hoped.

# More serious model comparison

Now it's time to ask whether the differences among models are substantial and reliable enough for us to prefer any one model over another.


```{r}
all_beliefs <- nodes(net.struct)

cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

belief_targets_models <- lapply(all_beliefs, function(b){
  
  f <- paste0("interv ~ ", b)

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(as.formula(f), 
                    data = interv_df,
                    family = "binomial"
                    )
  
  model <- add_node(cog_model, glm_to_jags_eq(interv_fit)) 
  
  }
)

model_compare <- tibble(
  model_name = all_beliefs,
  model = belief_targets_models
  ) %>%
    mutate(predictions = map(model, predict_change_scores_pipe)) %>%
    mutate(proc_preds = map(predictions, process_predictions, obs_data = obs_data)) %>%
    mutate(scores = map(proc_preds, score_predictions_boot)) %>%
    mutate(rmse = map(scores, ~ sapply(.x, function(z){z$rmse}))) %>%
  mutate(
    mean_rmse = map_dbl(rmse, mean),
    se_rmse = map_dbl(rmse, sd)
    )

model_compare %>%
  mutate(
    UL = mean_rmse + se_rmse,
    LL = mean_rmse - se_rmse
  ) %>%
  ggplot(aes(x = model_name, y = mean_rmse, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

## comparing alternate structures

```{r}
# here I can swap things around
mynet <- net.struct

mynet1 <- reverse.arc(mynet, "hb","nat") # this might be the best
mynet2 <- reverse.arc(mynet, "vaccStrain","vaccDanger") 
mynet3 <- reverse.arc(mynet1, "vaccStrain","vaccDanger")
mynet4 <- reverse.arc(mynet2, "vaccTox","vaccDanger")
mynet5 <- reverse.arc(mynet3, "vaccTox","vaccDanger")

possible_dags <- list(
  mynet,
  mynet1,
  mynet2,
  mynet3,
  mynet4,
  mynet5
  )

dag_comparison_models <- lapply(possible_dags, function(dag){
  
  dag_model <- write_jags_model(bnlearn_to_df(dag), d_bn) %>% suppressWarnings()

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(interv ~ diseaseSevere, 
                    data = interv_df,
                    family = "binomial"
                    )
  
  dag_model <- add_node(dag_model, glm_to_jags_eq(interv_fit)) 
  
  }
)

model_names <- c(
  "original",
  "swap_hb_nat",
  "swap_vs_vd",
  "swap_hb_nat_vs_vd",
  "swap_vs_vd_vt_vd",
  "swap_hb_nat_vs_vd_vt_vd"
)


model_compare_dags <- tibble(
  model_name = model_names,
  model = dag_comparison_models
  ) %>%
    mutate(predictions = map(model, predict_change_scores_pipe)) %>%
    mutate(proc_preds = map(predictions, process_predictions, obs_data = obs_data)) %>%
    mutate(scores = map(proc_preds, score_predictions_boot)) %>%
    mutate(rmse = map(scores, ~ sapply(.x, function(z){z$rmse}))) %>%
  mutate(
    mean_rmse = map_dbl(rmse, mean),
    se_rmse = map_dbl(rmse, sd)
    )

model_compare_dags %>%
  mutate(
    UL = mean_rmse + se_rmse,
    LL = mean_rmse - se_rmse
  ) %>%
  ggplot(aes(x = model_name, y = mean_rmse, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


----------------------------------------

# Broken?

Here I wrote some code to compute mse and mae across all individual subjects. I treated the resulting statistisc as means and calculated their standard error. I thought that would help clarify things, but actually it doesn't seem quite right as a model of the model predictions for average change scores. For instance, the disease severity targetting model isn't strongly preferred, despite getting like 20% more variance among average change scores.

```{r}

interv_df_wide <- interv_df %>%
  gather(scale, response, vaccIntent:parentExpert) %>%
  mutate(scale_id = paste0(scale,"_",phase)) %>%
  select(-phase, -scale, -condition, -interv) %>%
  spread(scale_id, response)

predict_change_scores_pipe <- partial(predict_change_scores,
                                      data0 = data.frame(interv=0),
                                      data1 = data.frame(interv=1),
                                      nodes_to_predict = nodes_to_predict,
                                      iter = 1e5
                                      )

compute_residuals <- function(predictions, observed){
  observed <- observed %>%
    as.matrix()
  
  predictions <- predictions %>% 
    select(-interv) %>%
      as.matrix()
  
  resids <- sweep(observed, 2, predictions)
  
}

compute_ms_residuals <- function(predictions, observed) {
  resids <- compute_residuals(predictions, observed)
  resids2 <- resids^2
  rowMeans(resids2)
}

compute_mabs_residuals <- function(predictions, observed) {
  resids <- compute_residuals(predictions, observed)
  # mae <- colMeans(abs(resids))
  rowMeans(abs(resids))
}

obs_data_indiv <- interv_df_wide %>%
            gather(scale_id, response, -workerId) %>%
          mutate(
            scale = gsub("_.*", "", scale_id),
            phase = gsub(".*_", "", scale_id)
          ) %>%
  select(-scale_id) %>%
  spread(phase, response) %>%
  mutate(change_score = post - pre) %>%
  select(-post, -pre) %>%
  spread(scale, change_score) %>%
  ungroup() %>%
  select(-workerId)
  

model_name_list <- c("cognitive model", "cognitive model\n(overpar)", "causal model")
model_list <- c(cog_model, cog_model2, causal_model)

model_compare <- tibble(
  model_name = model_name_list,
  model = model_list
  ) %>%
    mutate(predictions = map(model, predict_change_scores_pipe)) %>%
    mutate(
      abs_resids = map(predictions, compute_mabs_residuals, observed = obs_data_indiv),
      resids_squared = map(predictions, compute_ms_residuals, observed = obs_data_indiv)
           ) %>%
  gather(err_type, errs, abs_resids, resids_squared) %>%
  mutate(
    mean = map_dbl(errs, mean), 
    se = map_dbl(errs, ~ sd(.x)/sqrt(length(.x)-1))
    )

```

```{r}
model_compare %>%
  mutate(UL = mean + se,
         LL = mean - se) %>%
  ggplot(aes(x = model_name, y = mean, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  facet_wrap(~ err_type)
```


```{r}
# by individuals, not working great
all_beliefs <- nodes(net.struct)

cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

belief_targets_models <- lapply(all_beliefs, function(b){
  
  f <- paste0("interv ~ ", b)

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(as.formula(f), 
                    data = interv_df,
                    family = "binomial"
                    )
  
  model <- add_node(cog_model, glm_to_jags_eq(interv_fit)) 
  
  }
)

model_compare <- tibble(
  model_name = all_beliefs,
  model = belief_targets_models
  ) %>%
    mutate(predictions = map(model, predict_change_scores_pipe)) %>%
    mutate(
      abs_resids = map(predictions, compute_mabs_residuals, observed = obs_data_indiv),
      resids_squared = map(predictions, compute_ms_residuals, observed = obs_data_indiv)
           ) %>%
  gather(err_type, errs, abs_resids, resids_squared) %>%
  mutate(
    mean = map_dbl(errs, mean), 
    se = map_dbl(errs, ~ sd(.x)/sqrt(259))
    )
```

```{r}
model_compare %>%
  mutate(UL = mean + se,
         LL = mean - se) %>%
  ggplot(aes(x = model_name, y = mean, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  facet_wrap(~ err_type) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

To be completed ...
