---
title: 'Vaccines: Many Beliefs Study 3 - Simulations'
author: "Kara Weisman, Derek Powell"
date: "May 2, 2018"
output: 
  html_notebook: 
    code_folding: hide
---

```{r setup, include = F}
library(tidyverse)

```

__As of May 2, 2018 all text in this notebook is just copy/pasted from the study2 notebook, so should largely be disregarded. will fill this in later__

# A few initial thoughts

Fundamentally, there are two major questions we want to ask/answer:

1. How well can we predict the effect of the intervention on other beliefs using a previously-learned network (or by averaging over networks)?
2. Given what we know about the intervention effect, what is the best model of the belief system?

The first is an out-of-sample prediction question, the second is more a model fit question. The first can be answered by generating predictions from hydenet, or possibly even by generating predictions with BRMS and using rejection sampling (since just conditioning on the assigned condition variable). The second can be addressed with either a cross validation and prediction scheme, or with WAIC via BRMS, either using imputed data or using a two-stage fitting process where posteriors from stage 1 are passed in as priors in stage 2.

In this notebook, I'm going to focus on the first issue: predicting effects of interventions from learned networks.

```{r tidy, include = F}
# load data
d_all <- read.csv("../data/study2_data.csv")

# reformat to match previous analyses (i.e., 2 rows per participant)
d_demo <- d_all %>% 
  select(workerId, condition, gender, age, ethnicity, education, job, income,
         political_party, political_beliefs, eligible_pretest, 
         is_parent_posttest, children_num_posttest, children_oldest_posttest, 
         children_youngest_posttest, plan_parent_posttest,
         starts_with("flushot_"), starts_with("vax_"), starts_with("attention_"),
         starts_with("comments"), starts_with("duration"))
d_pre <- d_all %>% 
  select(workerId, ends_with("_pretest")) %>%
  select(-c(eligible_pretest, starts_with("flushot_"), starts_with("vax_"), 
            starts_with("attention_"), starts_with("comments"), 
            starts_with("duration"))) %>%
  rename_all(funs(gsub("_pretest", "", .))) %>%
  mutate(phase = "pre")
d_post <- d_all %>% 
  select(workerId, ends_with("_posttest")) %>%
  select(-c(is_parent_posttest, children_num_posttest, children_oldest_posttest,
            children_youngest_posttest, plan_parent_posttest,
            starts_with("flushot_"), starts_with("vax_"), starts_with("attention_"),
            starts_with("comments"), starts_with("duration"))) %>%
  rename_all(funs(gsub("_posttest", "", .))) %>%
  mutate(phase = "post")

d <- bind_rows(d_pre, d_post) %>% 
  gather(question, response, -c(workerId, phase)) %>%
  mutate(phase = factor(phase,
                        levels = c("pre", "post")),
         reverse_cat = ifelse(grepl("_[1-9]r$", question), TRUE, FALSE),
         # NOTE: "response" has already been reverse coded!
         question = factor(question),
         scale = factor(gsub("_.*$", "", question),
                        levels = c("vaccIntent", "vaccDanger", "vaccEff", 
                                   "vaccStrain", "vaccTox", 
                                   "diseaseSevere", "diseaseRare", 
                                   "infantImmLimCap", "infantImmWeak", 
                                   "medSkept", "hb", "nat", 
                                   "overpar", "parentExpert"))) %>%
  full_join(d_demo) %>%
  mutate(condition = factor(condition, levels = c("noInterv", "diseaseRisk"))) %>%
  filter(!is.na(response), !is.na(workerId), !is.na(condition)) %>%
  distinct()

# how many left?
d %>% distinct(workerId, condition) %>% count(condition)
```

```{r scores, include = F}
# score all scales
d_scored <- d %>%
  select(workerId, condition, phase, scale, response,
         gender:duration_posttest) %>%
  group_by(workerId, condition, phase, scale) %>%
  mutate(response = as.numeric(response)) %>%
  summarise(Mean = mean(response, na.rm = TRUE)) %>%
  ungroup() %>%
  distinct()
```


# Predicting scale averages

Let's see what the model predicts based on the change in the scale averages from pre-test to post-test. To transform this into something more like a real "fit", one approach is to predict a distribution of change scores and then compare the predicted and observed distributions.

### Disease Risk Condition

I'll generate predictions for each of the 13 other nodes with _diseaseSevere_ = pre-test mean and _diseaseSevere_ = post-test mean, and see how those other nodes change. This is like imagining that all that's changed between pre- and post-test is the information affecting diseaseSevere beliefs. So, conditioned only on what we know about how that changed, what should we expect would also change?

```{r}

summarize <- summarise
diseaseSevereMean0 <- d_scored %>% 
  filter(condition=="diseaseRisk", scale=="diseaseSevere", phase=="pre") %>% 
  summarize(mean = mean(Mean)) %>%
  as.numeric()

diseaseSevereMean1 <- d_scored %>% 
  filter(condition=="diseaseRisk", scale=="diseaseSevere", phase=="post") %>% 
  summarize(mean = mean(Mean)) %>%
  as.numeric()

```


Once we start visualizing the error inherent in the data and estimates, only the prediction for "diseaseRare", "overpar", and "vaxtox" lie more than 1 standard error away from the observed mean. The all-important "vaxIntent" prediction is right about 1 SE away from the observed mean.

That's part win for the model and part failure of the data to provide enough precision to really assess it strongly.


Below I do the actual inference with mcmc in jags, sampling from the multivariate distribution defined by the model graph.

```{r}
set.seed(123)
library(HydeNet)
library(rjags)
library(bnlearn)

# load in model from cogsci paper
# net.fit <- readRDS("net-fit-cogsci2018.rds")
net.struct <- readRDS("net-struct-cogsci2018.rds")

# load in data from cogsci paper
source("../study1/vacc_import_data.R")
source("../Scripts/gmod_tools.R")
# get model out of HYDE network

# # HERE WE CAN TEST DIFFERENT MODELS! THIS IS A QUICK HACK JUST TO SEE IF THINGS ARE SIMILAR
# net.struct <- mmhc(d_bn)

my_hyde_network <- bnlearn_to_hyde_string(net.struct)
my_hyde_network <- HydeNetwork(as.formula(my_hyde_network), data=as.data.frame(d_bn))

jags_model <- writeNetworkModel(my_hyde_network)

do_jags_inference <- function(model, variable.names, data = NULL, iter = 1e4, silent=TRUE) {
  if (silent) {
    pbar <- "none"
  } else {
    pbar <- "text"
  }
  
  jags <- jags.model(textConnection(model), data = data, quiet=silent)
  model_samples <- coda.samples(jags, variable.names, iter*2, progress.bar="none")
  return(model_samples)
}

make_predictions <- function(model, orig_data, nodes_to_predict, nodes_held_out=c(), iter=1e4) {
  # takes a model and dataframe and generates model predictions for variables listed in 
  # nodes_to_predict. By default, uses all remaining nodes in orig_data. Hold out nodes from
  # prediction with nodes_held_out. iter sets mcmc samples
  
  pred_df <- orig_data
  pred_df[,nodes_to_predict] <- NA
  
  for (i in 1:nrow(pred_df)) {
    row <- pred_df[i,]
    row[nodes_held_out] <- NA
    pred <- do_jags_inference(model, nodes_to_predict, data = row, iter = iter)
    point_preds <- rowMeans(t(pred[[1]][(iter/2+1):iter,nodes_to_predict]))
    pred_df[i,nodes_to_predict] <- point_preds
  }

  return(pred_df)
}

x1 <- as.data.frame(t(colMeans(d_bn)))
x2 <- as.data.frame(t(colMeans(d_bn)))


x1[,"diseaseSevere"] <- diseaseSevereMean0
x2[,"diseaseSevere"] <- diseaseSevereMean1

vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere") %>% distinct()
pred0 <- make_predictions(jags_model, x1, as.character(vars$scale),iter=1e5)
pred1 <- make_predictions(jags_model, x2, as.character(vars$scale),iter=1e5)

pred0 <- pred0 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "pre")
pred1 <- pred1 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "post")

predictions <- pred0 %>% 
  bind_rows(pred1) 

```

Recall, here's the network ...

```{r}
plot(my_hyde_network)
```

Predicted change scores against observed change scores (error bars = 95% CI) ...

```{r}
# make change score df with error info
# standard error function
stderr <- function(x) {
          sqrt(var(x[!is.na(x)]) / length(x[!is.na(x)]))
}


obsMeans <- d_scored %>% 
  filter(condition == "diseaseRisk") %>% 
  spread(phase, Mean) %>%
  mutate(score = post - pre) %>%
  group_by(scale) %>%
  summarize(change_score = mean(score),
            std_error = stderr(score)) %>%
  filter(scale!="diseaseSevere") %>%
  mutate(type="observed")

# %>% 
#   summarize(mean_resp = mean(Mean),
#             std_error = stderr(Mean)) %>% 

predMeans <- predictions %>% 
  group_by(scale,phase) %>% 
  summarize(mean_resp = mean(value)) %>%
  mutate(type="predicted") %>%
  spread(phase, mean_resp) %>%
  # group_by(scale) %>%
  mutate(change_score = post - pre,
            std_error = 0)

d_sim <- obsMeans %>% bind_rows(predMeans)

d_sim %>%
  mutate(
         UL = change_score + std_error*1.96,
         LL = change_score - std_error*1.96
         ) %>%
  ggplot(aes(y = reorder(scale, change_score),
             x = change_score,
             xmax = UL, 
             xmin = LL, 
             color = type)) +
  geom_point(shape=17, size=3) +
  geom_errorbarh(height=0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
simMean <- d_sim %>% 
  select(-post,-pre, -std_error) %>%
  filter(scale!="diseaseSevere") %>%
  group_by(type,scale) %>%
  spread(type, change_score)


simMeanCor <- cor(simMean$predicted, simMean$observed) %>% round(3)

simMean %>%
  ggplot(aes(x=predicted, y=observed)) +
  geom_point() +
  geom_text(aes(x=-.1, y= .2, label=paste("r =",simMeanCor))) +
  geom_smooth(method="lm") +
  labs(title="Average difference scores") +
  theme(aspect.ratio = 1)
```

# bootstrapping CI

Here I'm going to bootstrap a confidence interval for the correlation between model predictions and observations for aggregated change scores across the 13 predicted scales.

I was thinking that it's not quite right to do your standard "significance" test on this correlation, as these predicted/observed change scores aren't values sampled from a population of variables. Instead, they represent their own population, but their measurement is uncertain, with that uncertainty coming from the participant-level data.

As I reflect on this I'm potentially a bit out of my depth, statistically. But the idea was I would take bootstrap samples of the data, make model predictions for each, then calculate the correlation with obtained values.

The results give a bit more confidence about the estimate than you'd have from the CI you'd get just treating the 13 points as a sample from a population (.303 to .914).

```{r}
# watch out! this takes a good 20 min to run ...

vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere") %>% distinct()

boot_n <- 500
iters <- 1:boot_n
cors <- rep(NA,boot_n)

d_scored_wide <- d_scored %>%
  filter(condition=="diseaseRisk") %>%
  unite(all, scale, phase) %>%
  spread(all, Mean)

for (i in iters) {
  
  # resample d_scored as d_boot
  d_boot <- sample_n(d_scored_wide, size = nrow(d_scored_wide), replace=TRUE)
  
  x1 <- as.data.frame(t(colMeans(d_bn)))
  x2 <- x1
  
  # calculate mean from d_boot
  diseaseSevereMean0 <- d_boot %>% 
    summarize(mean=mean(diseaseSevere_pre)) %>%
    as.numeric()
  
  diseaseSevereMean1 <- d_boot %>% 
    summarize(mean=mean(diseaseSevere_post)) %>%
    as.numeric()
  
  x1[,"diseaseSevere"] <- diseaseSevereMean0
  x2[,"diseaseSevere"] <- diseaseSevereMean1
  
  # make predictions
  pred0 <- make_predictions(jags_model, x1, as.character(vars$scale), iter = 5*1e4)
  pred1 <- make_predictions(jags_model, x2, as.character(vars$scale), iter = 5*1e4)
  
  pred0 <- pred0 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase="pre")
  pred1 <- pred1 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase="post")
  
  predictions <- pred0 %>% 
    bind_rows(pred1) 
  
  # calculate observed change scores
  obsMeans <- d_boot %>%
    mutate(id = row_number()) %>% # all this to get a unique ID for each "participant"
    gather(scale, Mean,diseaseRare_post:vaccTox_pre) %>% 
    mutate(phase = ifelse(grepl("_post", scale),"post","pre")) %>%
    mutate(scale = gsub("_.*","",scale)) %>%
    spread(phase,Mean) %>%
    mutate(change = post-pre) %>%
    group_by(scale) %>%
    summarize(change_score = mean(change),
          std_error = stderr(change)) %>%
    filter(scale!="diseaseSevere") %>%
    mutate(type="observed")
  
  predMeans <- predictions %>% 
    group_by(scale,phase) %>% 
    summarize(mean_resp = mean(value)) %>%
    mutate(type="predicted") %>%
    spread(phase, mean_resp) %>%
    # group_by(scale) %>%
    mutate(change_score = post - pre,
              std_error = 0)
  # combine with predictions
  d_sim <- obsMeans %>% bind_rows(predMeans)
  
  simMean <- d_sim %>% 
    select(-post,-pre, -std_error) %>%
    filter(scale!="diseaseSevere") %>%
    group_by(type,scale) %>%
    spread(type, change_score)
  
  cors[i] <- cor(simMean$predicted, simMean$observed)
}


```

The mean correlation value is `r round(mean(cors),3)` and the bootstrapped 95% CI does not include zero. So, minimally, the model is picking up on _something_. Below is a histogram of correlation values, with a line marking the observed correlation in the original sample.

```{r}
mean(cors)
quantile(cors, c(.025,.975))

hist(cors, breaks = 20)
abline(v = .776, col="red")
```

# Modeling mean changes as causal model ...

```{r}
surgery_graph <- net.struct %>%
  drop.arc("vaccEff", "diseaseSevere") %>%
  drop.arc("infantImmLimCap", "diseaseSevere")

causal_network <- bnlearn_to_hyde_string(surgery_graph)
causal_network <- HydeNetwork(as.formula(causal_network), data=as.data.frame(d_bn))

jags_causal_model <- writeNetworkModel(causal_network)

x1 <- as.data.frame(t(colMeans(d_bn)))
x2 <- as.data.frame(t(colMeans(d_bn)))


x1[,"diseaseSevere"] <- diseaseSevereMean0
x2[,"diseaseSevere"] <- diseaseSevereMean1

vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere") %>% distinct()
pred0 <- make_predictions(jags_causal_model, x1, as.character(vars$scale),iter=1e5)
pred1 <- make_predictions(jags_causal_model, x2, as.character(vars$scale),iter=1e5)

pred0 <- pred0 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "pre")
pred1 <- pred1 %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "post")

predictions <- pred0 %>% 
  bind_rows(pred1) 

obsMeans <- d_scored %>% 
  filter(condition == "diseaseRisk") %>% 
  spread(phase, Mean) %>%
  mutate(score = post - pre) %>%
  group_by(scale) %>%
  summarize(change_score = mean(score),
            std_error = stderr(score)) %>%
  filter(scale!="diseaseSevere") %>%
  mutate(type="observed")

predMeans <- predictions %>% 
  group_by(scale,phase) %>% 
  summarize(mean_resp = mean(value)) %>%
  mutate(type="predicted") %>%
  spread(phase, mean_resp) %>%
  # group_by(scale) %>%
  mutate(change_score = post - pre,
            std_error = 0)

d_sim <- obsMeans %>% bind_rows(predMeans)

d_sim %>%
  mutate(
         UL = change_score + std_error*1.96,
         LL = change_score - std_error*1.96
         ) %>%
  ggplot(aes(y = reorder(scale, change_score),
             x = change_score,
             xmax = UL, 
             xmin = LL, 
             color = type)) +
  geom_point(shape=17, size=3) +
  geom_errorbarh(height=0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
simMean <- d_sim %>% 
  select(-post,-pre, -std_error) %>%
  filter(scale!="diseaseSevere") %>%
  group_by(type,scale) %>%
  spread(type, change_score)


simMeanCor <- cor(simMean$predicted, simMean$observed) %>% round(3)

simMean %>%
  ggplot(aes(x=predicted, y=observed)) +
  geom_point() +
  geom_text(aes(x=-.1, y= .2, label=paste("r =",simMeanCor))) +
  geom_smooth(method="lm") +
  labs(title="Average difference scores") +
  theme(aspect.ratio = 1)
```

The model does a very poor job of predicting change in attitudes if we treat it as a causal model.

# Baseline model (naïve bayes)

To evaluate the performance of our model, we should compare it to something else. I think a good baseline candidate is a "naïve bayes" or naïve causal model--where diseaseSevere predicts all other beliefs. This could be simulated by a series of linear regressions, but I think it will actually be easier to put it into hydenet.

Visually ...

```{r}
vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere", scale!="overpar") %>% distinct()

# naïve causal model, diseaseSevere--> all (equivalent change score prediction)
# nb_arcs <- data.frame(from = rep("diseaseSevere", 13), to = vars$scale)
nb_arcs <- data.frame(from = c(
  rep("diseaseSevere", 12),
  rep("overpar", 12)
  ), 
                      to = rep(vars$scale,2))

# # regression predicting vaccIntent (poor change score prediction)
# nb_arcs <- data.frame(from = names(d_bn), to = rep("vaccIntent", 14)) %>% filter(from!="vaccIntent")

# # naïve bayes, vacc intent "classifier" (slightly weaker change score prediction)
# nb_arcs <- data.frame(to = names(d_bn), from = rep("vaccIntent", 14)) %>% filter(to!="vaccIntent")


hyde_nb <- df_to_hyde_string(nb_arcs)
nb_hyde_network <- HydeNetwork(as.formula(hyde_nb), data=as.data.frame(d_bn))

nb_jags_model <- writeNetworkModel(nb_hyde_network)

plot(nb_hyde_network)
```



```{r}

x1 <- as.data.frame(t(colMeans(d_bn)))
x2 <- as.data.frame(t(colMeans(d_bn)))

x1[,"diseaseSevere"] <- diseaseSevereMean0
x2[,"diseaseSevere"] <- diseaseSevereMean1

pred0_nb <- make_predictions(nb_jags_model, x1, as.character(vars$scale),iter=1e5)
pred1_nb <- make_predictions(nb_jags_model, x2, as.character(vars$scale),iter=1e5)

pred0_nb <- pred0_nb %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "pre")
pred1_nb <- pred1_nb %>% as_tibble() %>% gather(scale, value) %>% mutate(phase = "post")

predictions_nb <- pred0_nb %>% 
  bind_rows(pred1_nb) 

obsMeans <- d_scored %>% 
  filter(condition == "diseaseRisk") %>% 
  spread(phase, Mean) %>%
  mutate(score = post - pre) %>%
  group_by(scale) %>%
  summarize(change_score = mean(score),
            std_error = stderr(score)) %>%
  filter(scale!="diseaseSevere") %>%
  mutate(type="observed")

predMeans_nb <- predictions_nb %>% 
  group_by(scale,phase) %>% 
  summarize(mean_resp = mean(value)) %>%
  mutate(type="predicted") %>%
  spread(phase, mean_resp) %>%
  # group_by(scale) %>%
  mutate(change_score = post - pre,
            std_error = 0)

d_sim_nb <- obsMeans %>% bind_rows(predMeans_nb)

d_sim_nb %>%
  mutate(
         UL = change_score + std_error*1.96,
         LL = change_score - std_error*1.96
         ) %>%
  ggplot(aes(y = reorder(scale, change_score),
             x = change_score,
             xmax = UL, 
             xmin = LL, 
             color = type)) +
  geom_point(shape=17, size=3) +
  geom_errorbarh(height=0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
simMean_nb <- d_sim_nb %>% 
  select(-post,-pre, -std_error) %>%
  filter(scale!="diseaseSevere") %>%
  group_by(type,scale) %>%
  spread(type, change_score)


simMeanCor <- cor(simMean_nb$predicted, simMean_nb$observed) %>% round(3)

simMean_nb %>%
  ggplot(aes(x=predicted, y=observed)) +
  geom_point() +
  geom_text(aes(x=-.1, y= .2, label=paste("r =",simMeanCor))) +
  geom_smooth(method="lm") +
  labs(title="Average difference scores") +
  theme(aspect.ratio = 1)
```

And performance is essentially identical. That is, __if we just naïvely assume disease severity drives all beliefs and we get just about as good a prediction as our more sophisticated model.__

Of course, our learned structure accounts for the original correlational data far (infinitely?) better. As revealed by a $\Delta AIC$ in the thousands.

```{r}
# comparing models on the original data
# maddeningly, bnlearn's AIC values are rescaled by -2, so that greater values are better!

nb_struct <- df_to_bnlearn_string(nb_arcs)
nb_struct <- model2network(nb_struct)

print(paste0("naïve structure AIC = ", round(AIC(nb_struct, d_bn)),2))
print(paste0("learned structure AIC = ", round(AIC(net.struct, d_bn),2)))
```


# Preliminary conclusions

On some reflection, these results shoudln't have been so shocking. Since the structure learning algorithm is working to find a graph that preserves the correlations among variables, it's not so surprising that conditioning on disease severity in the graph produces roughly the same predictions as a set of regression models predicting all other node from disease severity (or equivalently, the "naïve" network I used here). However, the fact that those predictions map reasonably well onto what we observe is still important!

__What's better about the network structure models compared with naïve or univariate models?__

1. Fit both correlations between all scales and predict change scores on all scales
2. Predict effects of educational interventions on any node or multiple nodes, all in one model
3. Give ways to help diagnose which nodes were directly affected by interventions
4. (Arguably) better allow interpretation of overall structure of belief system, better provide insights, etc
5. Serve as a cognitive model of the belief system in important ways

If we have three models, one that lets us do prediction in one domain, another that lets us to do prediction in another domain, and one that does both -- we should prefer the one that does both. Likewise, if we have three models, one that lets us do prediction, one that provides an intelligible cognitive theory, and one that does both--we should prefer the one that does both. So it would have been (or would be) a lot more exciting if the theory models could outperform the naïve models on both tasks, but it's still interesting/cool if they do at least as well on multiple tasks as the other models each do on their one task. 

That final point is important too. The (presumed) fact that the model will do better prediction treated as a cognitive model than as a causal model is very informative. In a sense, it's actually supported by the naïve models too. A naïve model where disease severity predicts all beliefs is a very poor model of the overall system. But, it is a decent model of how the beliefs change. This would be _very unlikely_ if what we were modeling was a simple causal system. 

This also suggests a more general lesson: If beliefs are sufficiently interconnected and there are few v-structures, then the directions of each edge may matter very little in terms of prediction. Of course, this also means that setting those edge directions (if we care to do so), is going to be difficult. It might also mean it's not the right way of concieving of the relationships among beliefs. 

