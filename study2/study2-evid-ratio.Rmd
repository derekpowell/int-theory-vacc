---
title: "R Notebook"
output: html_notebook
---

```{r setup, include = F}
library(tidyverse)
library(psych)
# library(ggcorrplot)
library(lme4)
library(brms)
library(rms)
```

# Data prep

```{r tidy, include = F}
source("load-study2-data.R", chdir = TRUE)
```

# doing it

```{r}
rescale_beta <- function(x, lower, upper) {
  # rescales onto the open interval (0,1)
  # rescales over theoretical bounds of measurement, specified by "upper" and "lower"
  # based on Smithson & Verkuilen (2006), though this is not as principled as you might think
  # see http://dx.doi.org/10.1037/1082-989X.11.1.54.supp

  N <- length(x)
  res <- (x-lower)/(upper - lower)
  res <- (res*(N-1) + .5)/N

  return(as.vector(res))
}

d <- d_scored %>%
  filter(scale == "diseaseSevere") %>%
  mutate(mean = rescale_beta(mean, -3, 3)) %>%
  spread(phase, mean) %>% 
  mutate(condition = relevel(condition, ref="noInterv")) %>%
  mutate(evid = ifelse(condition=="noInterv",0,1))
```

```{r}
brm_fitodds <- brm(
  bf(
    post ~ log(pre/(1-pre)) + evid*logER1 + logER0,
    logER1 + logER0 ~ 1,
    nl = TRUE,
    family = Beta("logit")
     ),
  prior = prior("normal(0,3)", nlpar = "logER1") +
          prior("normal(0,3)", nlpar = "logER0"),
    # prior("uniform(0,1)", lb = 0, ub = 1, nlpar = "p0"),
  data = d,
  iter = 4000,
  warmup = 2000,
  chains = 2,
  cores = 2,
  control = list(adapt_delta=.95)
)

# brm_fitodds <- brm(
#   bf(
#     post ~ logit(pre) + evid*logER1,
#     logER1 ~ 1,
#     nl = TRUE,
#     family = Beta("logit")
#      ),
#   prior = prior("normal(0,3)", nlpar = "logER1"), 
#     # prior("uniform(0,1)", lb = 0, ub = 1, nlpar = "p0"),
#   data = d,
#   iter = 4000,
#   warmup = 2000,
#   chains = 2,
#   cores = 2,
#   control = list(adapt_delta=.95)
# )

summary(brm_fitodds)
```

```{r}
pred_data <- expand.grid(
  list(
    pre = seq(.01,.99,.01),
    evid = c(0,1)
  )
)

predict(brm_fitodds, 
        pred_data, probs=c(.05,.95)) %>%
  as_tibble() %>%
  bind_cols(pred_data) %>%
  ggplot(aes(x=pre, y=Estimate, color = factor(evid), fill = factor(evid))) +
    geom_line() +
  geom_ribbon(aes(ymin = `5%ile`, ymax = `95%ile`, color=NULL), alpha=.2) +
  geom_jitter(data=d, aes(x=pre, y=post, color=factor(evid))) +
  theme_bw()
```

```{r}
evid_ratio <- exp(fixef(brm_fitodds)[1,1])
broom::tidy(brm_fitodds) %>%
  filter(grepl("b_", term)) %>%
  mutate(evid_ratio = exp(estimate))
```

The resulting evidence ratio is `r evid_ratio`. Reasonable CPT values would be .5 and `r .5*evid_ratio` That's not that huge really ...


# maximum likelihood estimate with optim()

WE can do this with optim too, which will work better when we go to do it 14 times.

```{r}
inv_logit <- plogis
logit <- qlogis

evid_score_func <- function(data, par) {
  
  inv_logit <- plogis
  logit <- qlogis

  pred_y <- with(data,
                 {
                   inv_logit(logit(pre) + evid*par[2] + par[1])
                 }
       )
  pred_beta_A <- pred_y * par[3]
  pred_beta_B <- (1-pred_y) * par[3]
  
  ll <- -1 * sum(dbeta(data$post, pred_beta_A, pred_beta_B, log=TRUE)) # beta regression
  return(ll)
}

get_evid_probs <- function(result){
  # this is a heuristic/hack, could be made better but probably doesn't matter
  evid_ratio <- exp(result$par[1] + result$par[2])
  if (evid_ratio < 1) {
    p0 <- .90
  } else {
    p0 <- .50
  }
  
  if (p0*evid_ratio > 1) {
    p0 <- p0*.9/(p0*evid_ratio)
  }

  p1 <- p0*evid_ratio
  
  c(p0,p1)
}


evid_probs_to_cpt <- function(probs, var_name){
  p0 <- probs[1]
  p1 <- probs[2]
  
  full_list <- paste0("list(",var_name,"=c('Yes','No'), evid = c('Yes', 'No'))")
  make_custom_cpt(c(p1, p0, (1-p1), (1-p0)), c(2,2), eval(parse(text=full_list)), NULL)
}


find_evid_ratio <- function(data){
  optim(par = c(0, 0, 5), fn = evid_score_func, data = data, method="BFGS")
}


make_evid_cpt <- function(data, var_name){
  result <- find_evid_ratio(data)
  probs <- get_evid_probs(result)
  cpt <- evid_probs_to_cpt(probs, var_name)
  
  return(cpt)
}

evid_cpt <- make_evid_cpt(d, "diseaseSevere")

```

