---
title: "R Notebook"
output: 
  html_notebook: 
    code_folding: hide
    toc: true
    toc_float: true
---



```{r load in existing objects, message=FALSE, warning=FALSE}
# net.struct <- readRDS("net-struct-cogsci2018.rds")

source("../study1/vacc_import_data.R")
source("../scripts/gmod_tools.R")
source("../scripts/virtualEvidence.R")

library(bnlearn)
library(rjags)
library(patchwork)
```

```{r load and tidy study 3 data, include = F}
# load data
d_all <- read.csv("../data/study2_data.csv")

# reformat to match previous analyses (i.e., 2 rows per participant)
d_demo <- d_all %>% 
  select(workerId, condition, gender, age, ethnicity, education, job, income,
         political_party, political_beliefs, eligible_pretest, 
         is_parent_posttest, children_num_posttest, children_oldest_posttest, 
         children_youngest_posttest, plan_parent_posttest,
         starts_with("flushot_"), starts_with("vax_"), starts_with("attention_"),
         starts_with("comments"), starts_with("duration"))
d_pre <- d_all %>% 
  select(workerId, ends_with("_pretest")) %>%
  select(-c(eligible_pretest, starts_with("flushot_"), starts_with("vax_"), 
            starts_with("attention_"), starts_with("comments"), 
            starts_with("duration"))) %>%
  rename_all(funs(gsub("_pretest", "", .))) %>%
  mutate(phase = "pre")
d_post <- d_all %>% 
  select(workerId, ends_with("_posttest")) %>%
  select(-c(is_parent_posttest, children_num_posttest, children_oldest_posttest,
            children_youngest_posttest, plan_parent_posttest,
            starts_with("flushot_"), starts_with("vax_"), starts_with("attention_"),
            starts_with("comments"), starts_with("duration"))) %>%
  rename_all(funs(gsub("_posttest", "", .))) %>%
  mutate(phase = "post")

d <- bind_rows(d_pre, d_post) %>% 
  gather(question, response, -c(workerId, phase)) %>%
  mutate(phase = factor(phase,
                        levels = c("pre", "post")),
         reverse_cat = ifelse(grepl("_[1-9]r$", question), TRUE, FALSE),
         # NOTE: "response" has already been reverse coded!
         question = factor(question),
         scale = factor(gsub("_.*$", "", question),
                        levels = c("vaccIntent", "vaccDanger", "vaccEff", 
                                   "vaccStrain", "vaccTox", 
                                   "diseaseSevere", "diseaseRare", 
                                   "infantImmLimCap", "infantImmWeak", 
                                   "medSkept", "hb", "nat", 
                                   "overpar", "parentExpert"))) %>%
  full_join(d_demo) %>%
  mutate(condition = factor(condition, levels = c("noInterv", "diseaseRisk"))) %>%
  filter(!is.na(response), !is.na(workerId), !is.na(condition)) %>%
  distinct()

# how many left?
d %>% distinct(workerId, condition) %>% count(condition)

# score all scales
d_scored <- d %>%
  select(workerId, condition, phase, scale, response,
         gender:duration_posttest) %>%
  group_by(workerId, condition, phase, scale) %>%
  mutate(response = as.numeric(response)) %>%
  summarise(Mean = mean(response, na.rm = TRUE)) %>%
  ungroup() %>%
  distinct()

stderr <- function(x) {
          sqrt(var(x[!is.na(x)]) / length(x[!is.na(x)]))
}


interv_df <- d_scored %>% 
              filter(condition=="diseaseRisk") %>%
              mutate(interv = ifelse(phase=="pre",0,1)) %>%
              group_by(workerId) %>%
              spread(scale, Mean)

obs_data <- d_scored %>% 
  filter(condition == "diseaseRisk") %>% 
  spread(phase, Mean) %>%
  mutate(score = post - pre) %>%
  group_by(scale) %>%
  summarize(
    observed = mean(score),
    std_error = stderr(score))
```

```{r}
# define some functions for doing and processing predictions

predict_change_scores <- function(model, data0, data1, nodes_to_predict, held_out_nodes=c(), iter=1e4) {
  # note: intervention must be named "interv"
  
  require(dplyr)
  
  preds_mean1 <- make_predictions(model, 
                                data1, 
                                nodes_to_predict, 
                                held_out_nodes,
                                iter = iter)

  preds_mean0 <- make_predictions(model, 
                                  data0, 
                                  nodes_to_predict, 
                                  held_out_nodes,
                                  iter = iter)
  
  preds_mean <- preds_mean1 - preds_mean0

}

process_predictions <- function(predictions, obs_data) {
  require(dplyr)
  predictions <- predictions %>% 
  select(-interv) %>%
  gather(scale, predicted) %>%
  left_join(obs_data, by = "scale") 
}

plot_processed_predictions <- function(processed_predictions) {
  require(dplyr)
  require(ggplot2)
  plt1 <- processed_predictions %>%
    mutate(
           UL = observed + std_error*1.96,
           LL = observed - std_error*1.96
           ) %>%
    ggplot(aes(x=predicted, y = observed, ymin = LL, ymax=UL)) +
    geom_errorbar(color="grey", width=0) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color="grey") +
    theme_bw() +
    theme(aspect.ratio = 1)

  plt2 <- processed_predictions %>% 
    # left_join(obs_error, by = "scale") %>%
    group_by(scale) %>%
    gather(type, change_score, observed, predicted) %>%
    mutate(std_error = ifelse(type == "observed", std_error, NA )) %>%
    mutate(
           UL = change_score + std_error*1.96,
           LL = change_score - std_error*1.96
           ) %>%
    ggplot(aes(y = reorder(scale, change_score),
               x = change_score,
               xmax = UL, 
               xmin = LL, 
               color = type)) +
    geom_point(shape=17, size=3) +
    geom_errorbarh(height=0) +
    theme_bw()
    # theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  return(list(corr_plot = plt1, means_plot = plt2))
}

score_predictions <- function(processed_predictions) {
  list(
  r = cor(processed_predictions$observed, processed_predictions$predicted),
  r_squared = cor(processed_predictions$observed, processed_predictions$predicted)^2,
  mae = mean(abs(processed_predictions$observed - processed_predictions$predicted)),
  rmse = sqrt(mean((processed_predictions$observed - processed_predictions$predicted)^2))
  )
}

score_predictions_boot <- function(processed_predictions) {
  
  samps <- 1:1000
  
  preds <- lapply(samps, function(x){
    pp <-  processed_predictions %>% 
      mutate(resamp_observed = rnorm(14, observed, std_error))
    list(
    r = cor(pp$resamp_observed, pp$predicted),
    r_squared = cor(pp$resamp_observed, pp$predicted)^2,
    mae = mean(abs(pp$resamp_observed - pp$predicted)),
    rmse = sqrt(mean((pp$resamp_observed - pp$predicted)^2))
    )
  })
  
  
}

```

# conceptual illustration

```{r}
p_x <- rbeta(100, 2, 2)
p_y_x0 = .4
p_y_x1 = .8

p_y = rethinking::rbeta2(100, p_x*p_y_x1 + (1-p_x)*p_y_x0, 100)

plot(p_x, p_y)

df <- data.frame(x=p_x, y=p_y)
df %>%
  rename(`p(fire)` = x,
         `p(smoke)` = y) %>%
  ggplot(aes(x=`p(fire)`, y = `p(smoke)`)) +
  geom_point() +
  theme_bw(base_size = 24)
```


# correlation matrix

```{r}
# borrowing code from
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}
  

corr_plot <- d_bn %>%
  cor() %>%
  reorder_cormat() %>%
  # get_lower_tri() %>%
  reshape2::melt(na.rm=TRUE) %>%
  ggplot(aes(x=Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_tile() +
  geom_text(aes(label=round(value,2)), size=6*.35) +
  scale_fill_distiller(type="div", palette = "RdYlBu") +
  # scale_fill_viridis_c(limit = c(-1,1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=.85)) +
  theme(aspect.ratio = 1) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(fill="corr.")

ggsave("corrplot.png", corr_plot, dpi=320, height=5,width=5)
```

# Aggregate predictions

This notebook aims to examine/address the issue of predicting out-of-sample data using our model. In a previous notebook, `vacc_manyBelief_study3_modelsim.RMD`, I predicted average scale responses from study 3 using the previously learned model by conditioning on the Disease Severe node. 

In this notebook, I'll ... 

1. Explicitly model the intervention, rather than just conditioning disease severe.
2. Examine model as "causal model"
3. Explore model predictions under different assumptions about what beliefs are "targetted" by the intervention.
4. Similarly, explore whether we should model the intervention as providing direct evidence for multiple beliefs.
5. Consider na√Øve models ...


## cognitive model

First, let's examine the predictions of our cognitive model, where we treat the graphical model as a cognitive model, and reading the intervention materials as observing or conditioning on some evidence relevant to beliefs about disease severity.

To recap, we have ...

1. The cognitive network model (from study 1, n=1130 (data = `d_bn`))
2. A model of the relationship between disease severity and the intervention, modeled from study 2.

And we're predicting the average change scores observed in each condition following the disease risk manipulation in Study 2. So, for the most part this is an out-of-sample prediction task -- predicting change scores for new participants on the basis of a model generated from correlations observed among different participants, along with a bit of info about the intervention's effects on disease severity from the same participants.

```{r, message=FALSE, warning=FALSE}

nodes_to_predict <- bnlearn::nodes(net.struct)


cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn)

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

cog_model <- add_node(cog_model, glm_to_jags_eq(interv_fit))

# can also just use "model" here so long as you include held_out_nodes

preds_cog <- predict_change_scores(cog_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5*2.5
                                           )

# preds_cog$interv <- 0

preds_cog <- process_predictions(preds_cog, obs_data)

plots_cog <- plot_processed_predictions(preds_cog)


scores_cog <- score_predictions(preds_cog)

plt1 <- plots_cog$corr_plot +
  geom_text(
    aes(
      label = ifelse(observed == preds_cog[which(preds_cog$scale == "diseaseSevere"), "observed"],
        as.character(scale), ""
      )
    ),
    hjust = 1.1, vjust = 0
  ) +
  geom_text(aes(label = ifelse(observed == preds_cog[which(preds_cog$scale == "overpar"), "observed"],
    as.character(scale), ""
  )), hjust = 1.1, vjust = 0) +
  theme(aspect.ratio = 1) +
  coord_cartesian(xlim=c(-.25,.35), ylim=c(-.25,.35)) +
  annotate("text", 
           x = .25, 
           y = -.2, 
           label = paste("italic(R)^2 == ", round(scores_cog$r_squared,3)), 
           parse = TRUE,
           size=18*.35
          ) +
  theme_bw(base_size=18)

plt2 <- plots_cog$means_plot +
  labs(x="Change score", y="Belief Scale") +
  theme_bw(base_size=18) +
  theme(legend.position = "bottom")

ggsave("plot.png", plot = plt1, dpi=320, width=4, height=4)
ggsave("plot2.png", plot = plt2, dpi=320, width=6, height=4)

```

So it's a pretty good fit, as we expected. Note that the r = `r score_predictions(preds_cog)$r %>% round(3)` is higher than the correlation in `...modelsims.Rmd` notebook, because I'm here also including disease severity in calculating the fit.

## "Non-mental" model

Next we can rule out the (perhaps not-all-that tempting) argument that we haven't developed a cognitive model at all, instead perhaps a causal model describing how holding different beliefs _causes_ a person to hold other beliefs. verbally that might not sound all that different, but the difference comes in how we conceive of evidence affecting participants' beliefs. Instead of an edge directed from the belief to the evidence, it'd be an edge directed from the evidence to the belief. 

(Here I'm fudging things a bit in a way that should be fine considering these are linear models. I'm keeping the edge directed from disease severity to intervention, but then doing "graph surgery" to prune the disease severity node's parents when we predict the effects of the intervention. Since we're predicting changes, I'm about 99% confident this should work.)

```{r, message=FALSE, warning=FALSE}
library(bnlearn)
surgery_graph <- net.struct %>%
  drop.arc("vaccEff", "diseaseSevere") %>%
  drop.arc("infantImmLimCap", "diseaseSevere")

causal_arcs <- bnlearn_to_df(surgery_graph)

causal_model <- write_jags_model(causal_arcs, d_bn) %>% suppressWarnings()

interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

causal_model <- add_node(causal_model, glm_to_jags_eq(interv_fit)) # this is a cheat but it's ok ...

preds_causal <- predict_change_scores(causal_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5
                                           )

preds_causal <- process_predictions(preds_causal, obs_data)

plots_causal <- plot_processed_predictions(preds_causal)

scores_causal <- score_predictions(preds_causal)

plt3 <- plots_causal$corr_plot +
  geom_text(
    aes(
      label = ifelse(observed == preds_causal[which(preds_causal$scale == "diseaseSevere"), "observed"],
        as.character(scale), ""
      )
    ),
    hjust = 1.1, vjust = 0
  ) +
  # geom_text(aes(label = ifelse(observed == preds_cog[which(preds_cog$scale == "overpar"), "observed"],
  #   as.character(scale), ""
  # )), hjust = 1.1, vjust = 0) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  theme(aspect.ratio = 1) +
  coord_cartesian(xlim=c(-.25,.35), ylim=c(-.25,.35)) +
  annotate("text", 
           x = .25, 
           y = -.2, 
           label = paste("italic(R)^2 == ", round(scores_causal$r_squared,3)), 
           parse=TRUE,
           size=14*.35
          ) +
  theme_bw(base_size=18)

# plots_causal$means_plot +
#   labs(x="Change score", y="Belief Scale") +
#   theme_bw(base_size=18)

ggsave("plot3.png", plot = plt3, dpi=320, width=4, height=4)
```

Treating things as a causal model makes very poor predictions.

# What beliefs are targetted by the intervention?

Here we can pretend that expectations for the intervention materials are set by each of the 14 beliefs, and compare the accuracy of the resulting models' predictions. We should also ask whether the differences among models are substantial and reliable enough for us to prefer any one model over another.


```{r}
predict_change_scores_pipe <- partial(predict_change_scores,
                                      data0 = data.frame(interv = 0),
                                      data1 = data.frame(interv = 1),
                                      nodes_to_predict = nodes_to_predict,
                                      iter = 1e5
                                      )


all_beliefs <- nodes(net.struct)

cog_model <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

belief_targets_models <- lapply(all_beliefs, function(b){
  
  f <- paste0("interv ~ ", b)

  # can modify this to test assumptions about direct effects of intervention ...
  interv_fit <- glm(as.formula(f), 
                    data = interv_df,
                    family = "binomial"
                    )
  
  model <- add_node(cog_model, glm_to_jags_eq(interv_fit)) 
  
  }
)

model_compare <- tibble(
  model_name = all_beliefs,
  model = unlist(belief_targets_models)
  ) %>%
    mutate(predictions = map(model, predict_change_scores_pipe)) %>%
    mutate(proc_preds = map(predictions, process_predictions, obs_data = obs_data)) %>%
    mutate(scores = map(proc_preds, score_predictions_boot))

model_compare2 <- model_compare %>%
    mutate(
      rmse = map(scores, ~ sapply(.x, function(z){z$rmse})),
      r_squared = map(scores, ~ sapply(.x, function(z){z$r_squared}))
      ) %>%
  gather(metric, value, rmse, r_squared) %>%
  mutate(
    mean_value = map_dbl(value, mean),
    se_value = map_dbl(value, sd)
    )

plt_models <- model_compare2 %>%
  mutate(
    UL = mean_value + se_value,
    LL = mean_value - se_value
  ) %>%
  ggplot(aes(x = reorder(model_name, -mean_value), y = mean_value, ymin = LL, ymax = UL)) +
  geom_pointrange() +
  theme_bw(base_size=18) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(~metric, scales="free") +
  labs(x="Targetted belief", y = "Score")

ggsave("plot_models.png", plot = plt_models, dpi=320, width=12, height=6)
```



# Modeling multiple targetted beliefs

The biggest outlier point for our cognitive model is the "parental protectiveness" (`overpar`) belief. Considering the evocative content of the intervention, it's possible that it directly bears on this belief. 

Here we can model our intervention targetting both disease severity and overparenting ...

```{r, message=FALSE, warning=FALSE}

cog_model2 <- write_jags_model(bnlearn_to_df(net.struct), d_bn) %>% suppressWarnings()

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere + overpar, 
                  data = interv_df,
                  family = "binomial"
                  )

cog_model2 <- add_node(cog_model2, glm_to_jags_eq(interv_fit))

# can also just use "model" here so long as you include held_out_nodes

preds_cog2 <- predict_change_scores(cog_model2,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5*2.5
                                           )



preds_cog2 <- process_predictions(preds_cog2, obs_data)

plots_cog2 <- plot_processed_predictions(preds_cog2)

scores_cog2 <- score_predictions(preds_cog2)

plt4 <- plots_cog2$corr_plot +
  geom_text(
    aes(
      label = ifelse(observed == preds_cog2[which(preds_cog2$scale == "diseaseSevere"), "observed"],
        as.character(scale), ""
      )
    ),
    hjust = 1.1, vjust = 0
  ) +
  geom_text(aes(label = ifelse(observed == preds_cog2[which(preds_cog2$scale == "overpar"), "observed"],
    as.character(scale), ""
  )), hjust = 1.1, vjust = 0) +
    geom_text(aes(label = ifelse(observed == preds_cog2[which(preds_cog2$scale == "infantImmWeak"), "observed"],
    as.character(scale), ""
  )), hjust = 1.1, vjust = 0) +
  # geom_abline(intercept = 0, slope = 1, linetype = "dashed", color="grey") +
  # geom_errorbar(aes(ymin = LL, ymax=UL)) +
  theme(aspect.ratio = 1) +
  coord_cartesian(xlim=c(-.25,.35), ylim=c(-.25,.35)) +
  annotate("text", 
           x = .25, 
           y = -.2, 
           label = paste("italic(R)^2 == ", round(scores_cog2$r_squared,3)), 
           parse=TRUE,
           size=14*.35
          )

plots_cog2$means_plot +
  labs(x="Change score", y="Belief Scale")


ggsave("plot4.png", plot = plt4, dpi=320, width=4, height=4)
```

## Comparing models

This code is a bit hacky, should write some better bootstrapping code. But here is a quick comparison, with somewhat unclear results. Previously this looked more clear cut, to me suggesting there may be more volatility in the mcmc sampling than I had thought. I should maybe give them more burn-in.

```{r}
boot_scores_cog <- score_predictions_boot(preds_cog)
boot_scores_cog2 <- score_predictions_boot(preds_cog2)

tibble(cog = sapply(boot_scores_cog, function(x){x$rmse}), 
           cog2 = sapply(boot_scores_cog2, function(x){x$rmse})) %>%
  gather(model, rmse, cog, cog2) %>%
  ggplot(aes(x=model, y = rmse)) +
  geom_boxplot() +
  theme_bw()
```

# Na√Øve Models

Once we see that we have a cognitive model, I'll say the _cognitive model assumption_ is met. And when it's met, it actually has some interesting consequences. 

Because observations in a cognitive model (as in, exposure to educational materials) are conditioning events rather than causal interventions, we can actually do a good job predicting their effects using na√Øve models. What I mean by na√Øve models are models that na√Øvely assume all beliefs are connected to some central, focal belief(s)--specifically, whichever beliefs are targetted by educational interventions.

This works because all we need to predict the expected change is a model of the marginal conditional distribution of each variable. To put it in some math, our model approximates the joint distribution over all the variables $x_1$ through $x_n$:

$$P(x_1, x_2, x_3, ..., x_n)$$

But to ask how some other belief, $y$ will change on average, given some change in another belief $x$, that's something like this:

$$E[P(y|x=1) - P(y|x=0)]$$

And for that we only need to know the conditional probability, $P(y|x)$, which we can approximate in this na√Øve model. So here we have a model that na√Øvely assumes that disease severity beliefs influence all other beliefs.

```{r, message=FALSE, warning=FALSE}

vars <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere") %>% distinct()

# na√Øve causal model, diseaseSevere--> all (equivalent change score prediction)
nb_arcs <- data.frame(from = rep("diseaseSevere", 13), to = vars$scale)

nb_model <- write_jags_model(nb_arcs, d_bn) %>% suppressWarnings()


# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere, 
                  data = interv_df,
                  family = "binomial"
                  )

nb_model <- add_node(nb_model, glm_to_jags_eq(interv_fit))


# can also just use "model" here so long as you include held_out_nodes

preds_nb <- predict_change_scores(nb_model,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5
                                           )

preds_nb <- process_predictions(preds_nb, obs_data)

plots_nb <- plot_processed_predictions(preds_nb)

plots_nb$corr_plot
plots_nb$means_plot

score_predictions(preds_nb)

```

The na√Øve model is comparable to the more complex cognitive model, at least when it comes to predicting the changes in beliefs following the interventions. Of course, as a model of the actual relationships among beliefs, it's terrible (see `...modelSims.Rmd` notebook for a brief exploration of that).

### Na√Øvely modeling multiple targetted beliefs

We can also model multiple direct evidence relations by extending the na√Øve model. There are probably multiple ways to extend it. What I've come up with here is to just assume that both disease severity and parental protectiveness are the two focal beliefs, and that they both influene all other beliefs. I suppose I could also add a connection between them one way or another.

```{r, message=FALSE, warning=FALSE}

vars2 <- d_scored %>% select(scale) %>% filter(scale!="diseaseSevere", scale!="overpar") %>% distinct()

# na√Øve causal model, diseaseSevere--> all (equivalent change score prediction)

nb_arcs2 <- data.frame(
  from = c(
    rep("diseaseSevere", 12),
    rep("overpar", 12)
  ),
  to = rep(vars2$scale, 2)
)

nb_model2 <- write_jags_model(nb_arcs2, d_bn) %>% suppressWarnings()

# can modify this to test assumptions about direct effects of intervention ...
interv_fit <- glm(interv ~ diseaseSevere + overpar, 
                  data = interv_df,
                  family = "binomial"
                  )

nb_model2 <- add_node(nb_model2, glm_to_jags_eq(interv_fit))


# can also just use "model" here so long as you include held_out_nodes

preds_nb2 <- predict_change_scores(nb_model2,
                                           data.frame(interv = 0),
                                           data.frame(interv = 1),
                                           nodes_to_predict,
                                           iter = 1e5*2.5
                                           )

preds_nb2 <- process_predictions(preds_nb2, obs_data)

plots_nb2 <- plot_processed_predictions(preds_nb2)

plots_nb2$corr_plot
plots_nb2$means_plot

score_predictions(preds_nb2)

```

This model is the best of all!, getting a whopping `r score_predictions(preds_nb2)$r^2 %>% round(3) * 100`% of the variance! Not only that, RMSE and MAE also look somewhat lower than the other models. 

So, from a purely predictive standpoint, if the cognitive model assumption holds, then the na√Øve model might actually be the best approach to predicting belief changes. However I don't think there are any guarantees that this sort of thing will always work when thinking about multiple targetted beliefs, for one. Quickly comparing all four models, it really does look like the na√Øve model with both disease severity and overparenting targetted does best, for what it's worth.

```{r}
boot_scores_cog <- score_predictions_boot(preds_cog)
boot_scores_cog2 <- score_predictions_boot(preds_cog2)
boot_scores_nb <- score_predictions_boot(preds_nb)
boot_scores_nb2 <- score_predictions_boot(preds_nb2)

tibble(
  cog = sapply(boot_scores_cog, function(x){x$rmse}), 
  cog2 = sapply(boot_scores_cog2, function(x){x$rmse}),
  nb = sapply(boot_scores_nb, function(x){x$rmse}),
  nb2 = sapply(boot_scores_nb2, function(x){x$rmse}),
       ) %>%
  gather(model, rmse, cog, cog2, nb, nb2) %>%
  ggplot(aes(x=model, y = rmse)) +
  geom_boxplot() +
  theme_bw()
```
