---
title: "Vaccine toxins full experiment"
author: Derek Powell
date: April 9, 2019
output: 
  html_notebook: 
    code_folding: hide
---

NOTE: All ps eventually saw the intervention, those in no-intervention control just saw it after they responsed to the questions. I'm not fully sure what my logic was there--equity I guess. I may have also been thinking we could ask them things about their reaction to the intervention, but we never did that. On reflection now, I think it was a poor decision. But, it doesn't seem to explain our differential dropout rate since that's mostly due to peopel dropping out at the 18% mark.

### Loading packages and importing data

```{r message=FALSE}
library(tidyverse)
library(brms)

# pre <- read_qualtrics("../data_private/vacc+tox+interv+full+-+pre_April+3,+2019_15.38.csv")
# post <- read_qualtrics("../data_private/vacc+tox+interv+full+-+post_April+5,+2019_13.59.csv")
source("../code/helpers.R")
source("../code/load-data-s3ab.R")
```

## Preliminaries

This is a full-sized experiment testing our vaccine toxins intervention following a successful pilot. The experiment was conducted across two days, with pretest session on April 3th, 2019 and participants re-recruited to an intervention/posttest session the next day, April 4th, 2019. 

In part 1, we recruited `r nrow(pre %>% filter(Progress==100))` participants, of which `r nrow(df_pre)` passed check questions and were re-recruited for part 2. In part 2, `r nrow(post)` of those recruited joined the study, and `r nrow(post %>% filter(Progress==100))` finished it. Of these, `r nrow(df_post)` passed attention check questions and are included in the final data. Of these, `r nrow(df_post %>% filter(condition=="intervention"))` were assigned to the intervention condition and `r nrow(df_post %>% filter(condition=="noInterv"))` to a no-intervention control.

### Histograms

The distributions of responses at pretest appear very similar across conditions, and similar to those observed previously.

```{r}
s3_scored %>%
  ggplot(aes(x=pre, fill=condition)) + 
  geom_histogram(position="dodge",alpha=.7) + 
  facet_wrap(~scale) +
  theme_bw() +
  labs(title="Histogram of pretest scores", fill = "Condition", x = "Response")

```

### Scale correlations (pretest)

Correlations at pretest look very similar to those observed previously.

```{r fig.width=8, fig.height=8}

# borrowing code from
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}
  

s3_scored %>%
  select(workerId, condition, scale, pre) %>%
  spread(scale, pre) %>%
  select(-workerId, -condition) %>%
  cor() %>%
  reorder_cormat() %>%
  # get_lower_tri() %>%
  reshape2::melt(na.rm=TRUE) %>%
  ggplot(aes(x=Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_tile() +
  geom_text(aes(label=round(value,2))) +
  scale_fill_distiller(type="div", palette = "RdYlBu") +
  # scale_fill_viridis_c(limit = c(-1,1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1)) +
  theme(aspect.ratio = 1) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(fill="corr.")
# 
# 
# s3_scored %>%
#   select(workerId, condition, scale, pre) %>%
#   spread(scale, pre) %>%
#   select(-workerId, -condition) %>%
#   cor() %>%
#   as_tibble() %>%
#   mutate(x_scale = names(.)) %>%
#   gather(y_scale, correl, -x_scale) %>%
#   ggplot( aes(x = x_scale, y=y_scale, fill=correl)) +
#   geom_tile() +
#   scale_fill_distiller(palette="RdYlBu") +
#   theme_minimal() +
#   theme(aspect.ratio = 1) +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Change scores

I computed a change score for each belief scale as `change = posttest - pretest`. Below, I'm plotting the change scores for all 14 belief scales in the intervention and control conditions.

```{r}
s3_scored %>%
  group_by(condition, scale) %>%
  summarize(
    Mean = mean(change),
    se = sd(change)/sqrt(n())
    ) %>%
  mutate(
    ul = Mean + se,
    ll = Mean - se
  ) %>%
  ungroup() %>%
  mutate(condition = factor(condition, labels = c("Intervention","Control"))) %>%
  ggplot(aes(x=reorder(scale, Mean), color = condition, y = Mean, ymin = ll, ymax = ul)) +
  geom_errorbar(width=.1) +
  geom_point() +
  geom_hline(yintercept=0, linetype="dashed") +
  # facet_wrap(~scale) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# s3_scored %>%
#   ggplot(aes(x=scale, color = condition, fill = condition, y = change)) +
#   geom_violin() +
#   geom_hline(yintercept=0, linetype="dashed") +
#   # facet_wrap(~scale) +
#   theme_bw() +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
result <- s3_scored %>%
  filter(scale=="vaccTox") %>%
  group_by(condition) %>%
  summarize(
    Mean = mean(change),
    SD = sd(change)
  )

cohens_D <- function(m1, m2, s1, s2) {
  
  s_pooled <- sqrt((s1^2 + s2^2)/2)
  d <- (m1 - m2)/s_pooled
  
  return(d)
}

obs_d <- cohens_D(result[[1,2]], result[[2,2]], result[[1,3]], result[[2,3]])
```


__The Good News__: We replicate the large effect on vaccine toxins beliefs, and see significant changes across other beliefs as well. There is also a significant difference between intervention and control for vaccine intentions.

The effect has a cohen's D of `r round(obs_d,2)`, even larger than what we observed in our original pilot! (though that maybe due to slight increase in control condition).

__The Bad News__: The difference between conditions for vaccine intentions is driven by a decrease in the _control_ condition moreso than the increase in the intervention condition. Comparing scores just within the intervention condition against zero, the intervention is not significantly increasing intentions to vaccinate.

### Inspecting vaccine intentions

```{r}
fit <- cbrm(
  resp ~ phase * condition + (1 | workerId),
  data = s3_scored %>%
    filter(scale == "vaccIntent") %>%
    gather(phase, resp, post, pre) %>%
    mutate(resp = rescale_beta(resp, 1, 7)),
  family = Beta(),
  prior = prior(normal(0,1), class="b"),
  chains = 2,
  cores = 2,
  iter = 2000,
  seed = 1234,
  file = "../local/s3ab-fit-vaccIntent.Rds"
)

```


```{r}
t.test(s3_scored %>% filter(scale=="vaccIntent", condition=="intervention") %>% .$change)
```

In contrast, the difference in the control condition for vaccine intentions is really quite huge. A one-sample t-test remains significant after correcting for multiple comparisons (bonferroni adjusted $\alpha$ = .0036).

```{r}
t.test(s3_scored %>% filter(scale=="vaccIntent", condition=="noInterv") %>% .$change)
```

So unfortunately something really is amiss with these vaccine intentions responses. There are also some suspicious changes in a handful of other beliefs: vaccine toxicity, holistic balance, naturalism, and infant immune system weakness, but using a one-sample t-test, but none of those changes are significantly different from zero by conventional standards, even without correcting for multiple comaprisons.

#### Regression

Inspecting this as a regression of `posttest ~ pretest + condition` and plotting it doesn't reveal anything too suspicious-looking in terms of outliers etc, looks like a genuine decrease.

```{r}

summary(
  lm(post ~ pre + condition, data = s3_scored %>% filter(scale=="vaccDanger"))
)


s3_scored %>%
  filter(scale=="vaccIntent") %>%
  ggplot(aes(x=pre, y = post, color=condition)) +
  geom_jitter() + 
  theme_bw() +
  theme(aspect.ratio=1)
```

#### Pretest differences? 

Let's look at the pretest scores across the conditions, do they vary (by chance)? No significant pre-test differences, and if anything the difference shows the no-intervention group scoring lower at pretest, presumably offering the potential for more of an increase.

```{r}
pretest_scores <- s3_scored %>%
  filter(scale=="vaccIntent") %>%
  group_by(condition) %>%
  summarize(
    pre_mean = mean(pre),
    pre_sd = sd(pre),
    )

t.test(pre ~ condition, 
       data = s3_scored %>% filter(scale=="vaccIntent")
       )

s3_scored %>%
  filter(scale=="vaccIntent") %>%
  group_by(condition) %>%
  summarize(
    pre_mean = mean(pre),
    pre_se = sd(pre)/sqrt(n()),
    ) %>%
  mutate(
    ll = pre_mean - pre_se,
    ul = pre_mean + pre_se
  ) %>%
  ggplot(aes(x=condition, y=pre_mean, ymin=ll, ymax=ul)) +
  geom_pointrange() +
  theme_bw()
```

#### parents vs non-parents

Maybe if you're a parent who has already vaccinated, you can't change your beliefs much? If anything, looks like it's the other way around. Though, the non-parents seem to be showing a somewhat sharper decline.

```{r}
s3_scored %>%
  filter(scale=="vaccIntent") %>%
  filter(!is.na(parent)) %>%
  group_by(parent, condition) %>%
  summarize(mean_change = mean(change),
            se = sd(change)/sqrt(n()),
            N = n()
            ) %>%
  ungroup() %>%
  mutate(
    ll = mean_change - se,
    ul = mean_change + se,
    parent=factor(parent, labels=c("non-parents","parents"))
    ) %>%
  ggplot(aes(x=parent, y = mean_change, ymin = ll, ymax=ul, color=condition)) +
  geom_pointrange(position=position_dodge(width=.25)) +
  theme_bw()
```

### Did something genuinely happen between pretest and posstest?

So then the natural question is, did something happen between the two phases that could have changed paticipants attitudes? Or more generally, is there something fishy going on across the board? Clearly, it is somewhat odd that vaccine intentions shifted all by themselves, without attendant changes in highly-related beliefs about vaccine danger, efficacy, or disease severity (though there's some movement on toxicity).

So I'd like to test whether there is more going on overall than just the change in vaccine intentions, i.e., is it odd that we have 3 other scales also nearly significantly different from zero, or are those normal changes to expect. To test this, I'll use a bootstrapping approach. I'll compute the 14 observed t-scores and ask are there overall more extreme values than we would expect under a set of null results, in addition to the vaccIntent issue.


```{r}
# x <- t.test(s3_scored %>% filter(scale=="vaccIntent", condition=="noInterv") %>% .$change)

get_t_stat <- function(scale){
  data <- s3_scored %>% filter(scale==!!scale, condition=="noInterv")
  
  test <- t.test(data$change)
  
  return(test$statistic)
}

obs_F <- tibble(
  scale = unique(s3_scored$scale),
  t = map_dbl(scale, get_t_stat),
  F_stat = t^2
  ) %>%
  summarize(F_mean = mean(F_stat))

obs_F_noVacc <- tibble(
  scale = unique(s3_scored$scale),
  t = map_dbl(scale, get_t_stat),
  F_stat = t^2
  ) %>%
  filter(scale!="vaccIntent") %>% # comment
  summarize(F_mean = mean(F_stat))


tibble(x = rep(14,5000)) %>%
  mutate(F_mean = map_dbl(x, function(x){ mean(rt(x, 493)^2) }) ) %>%
  mutate(greater = ifelse(F_mean > obs_F$F_mean, 1, 0)) %>%
  summarize(p = mean(greater))

tibble(x = rep(13,5000)) %>%
  mutate(F_mean = map_dbl(x, function(x){ mean(rt(x, 493)^2) }) ) %>%
  mutate(greater = ifelse(F_mean > obs_F_noVacc$F_mean, 1, 0)) %>%
  summarize(p_no_vaccIntent = mean(greater))
```

Doing a bootstrap, things are overall more extreme than expected under the null across the 14 beliefs. If we remove the vaccIntent scale from consideration however, then things are no more extreme than expected.

Unfortunately that really just adds to the mystery--if there were systematic variations it would feel safe to conclude that "something happened" that affected resposnes in the control condition, suggesting these results are generally suspect. But it seems isolated to the vaccIntent scale. Nevertheless, it's problematic for our purposes.

## Comparison with model predictions

So there is something strange going on in the control group's responses for vaccine intentions which may prevent us from testing that main prediction properly. We can (maybe) still ask whether the changes across the other remaining beliefs are in line with the model's predictions.


```{r}
## ----------------------------------------

rescale_beta <- function(x, lower, upper) {
  # rescales onto the open interval (0,1)
  # rescales over theoretical bounds of measurement, specified by "upper" and "lower"

  N <- length(x)
  res <- (x - lower) / (upper - lower)
  res <- (res * (N - 1) + .5) / N

  return(as.vector(res))
}

d_vacctox <- s3_scored %>%
  select(-change) %>%
  gather(phase, mean, pre,post) %>%
  mutate(mean = mean-4) %>%
  ungroup()

## ----------------------------------------
## this was just to double check my optim() solution is right
# library(brms)
#
# brm_fitodds <- brm(
#   bf(
#     post ~ log(pre/(1-pre)) + evid*logER1,
#     logER1 ~ 1,
#     nl = TRUE,
#     family = Beta("logit")
#      ),
#   prior = prior("normal(0,3)", nlpar = "logER1"),
#           # prior("normal(0,3)", nlpar = "logER0"),
#     # prior("uniform(0,1)", lb = 0, ub = 1, nlpar = "p0"),
#   data = filter_dscored(d_vacctox,"vaccTox"),
#   iter = 4000,
#   warmup = 2000,
#   chains = 2,
#   cores = 2,
#   control = list(adapt_delta=.90)
# )
```


```{r}
## WARNING: THIS CODE IS HACKY!
## 
## script for generating model predictions based on vaccine toxins intervention
## run:
## int-theory-vacc/notebooks/s2-modeling.Rmd (to line 495)
##
## then run this notebook, and finally this code:

filter_dscored <- function(data, scale_name){
  
  data %>%
  filter(scale == scale_name) %>%
  mutate(mean = rescale_beta(mean, -3, 3)) %>%
  spread(phase, mean) %>% 
  mutate(condition = relevel(condition, ref="noInterv")) %>%
  mutate(evid = ifelse(condition=="noInterv",0,1))
}

# # update model fitting thing ignore control condition b/c of strange things in this data ...

find_evid_ratio <- function(data){
    optim(par = c(0, 5), fn = evid_score_func, data = data, method="BFGS")
}

# find_evid_ratio <- function(data){
#   # HACK HACK HACK
#   return (data.frame(par = c(-.55, 7)))
# }

evid_score_func <- function(data, par) {
    
    inv_logit <- plogis
    logit <- qlogis
    
    pred_y <- with(data,
                   {
                       inv_logit( logit(pre) + evid*par[1] )
                   }
    )
    pred_beta_A <- pred_y * par[2]
    pred_beta_B <- (1-pred_y) * par[2]
    
    ll <- -1 * sum(dbeta(data$post, pred_beta_A, pred_beta_B, log=TRUE)) # beta regression
    return(ll)
}

get_evid_probs <- function(result){
  # this is a heuristic/hack, could be made better but probably doesn't matter
  evid_ratio <- exp(result$par[1])
  if (evid_ratio < 1) {
    p0 <- .90
  } else {
    p0 <- .5
  }
  
  if (p0*evid_ratio >= 1) {
    p0 <- p0*.9/(p0*evid_ratio)
  }

  p1 <- p0*evid_ratio
  
  c(p0,p1)
}

predModel <- add_evid_node(filter_dscored(d_vacctox, "vaccTox"), base_model, "vaccTox")

pred0 <- sapply(nodes, function(x){
  statement <- paste0("cpquery(predModel, event = (", x, " == 'Yes'), evidence=TRUE, n = 5e5)")
  eval(parse(text=statement))
})

pred1 <- sapply(nodes, function(x){
  statement <- paste0("cpquery(predModel, event = (", x, " == 'Yes'), evidence= (evid == 'Yes'), n = 5e5)")
  eval(parse(text=statement))
})

pred_changes <- pred1 - pred0

pred_changes <- as.data.frame(pred_changes)

pred_changes <- pred_changes %>%
  mutate(scale = nodes) %>%
  rename(Mean = pred_changes) %>%
  mutate(type = "predicted")

saveRDS(object = pred_changes, file = "pred_changes.Rds")
```


```{r}

pred_changes <- readRDS("pred_changes.Rds")

preds_obs <- s3_scored %>%
  filter(condition == "intervention") %>%
  group_by(scale) %>%
  mutate(
    pre = rescale_beta((pre - 4), -3, 3),
    post = rescale_beta((post - 4), -3, 3)
  ) %>%
  mutate(change = post - pre) %>%
  summarize(
    Mean = mean(change),
    se = sd(change) / sqrt(n())
  ) %>%
  mutate(
    ul = Mean + se,
    ll = Mean - se
  ) %>%
  left_join(pred_changes %>% rename(pred_mean = Mean), by = "scale")

preds_obs %>%
  rename(
    observed = Mean,
    predicted = pred_mean
  ) %>%
  gather(type, Mean, observed, predicted) %>%
  ggplot(aes(y = reorder(scale, Mean), x = Mean, xmin = ll, xmax = ul, color=type)) +
  geom_point()

preds_obs %>%
  ggplot(aes(x=pred_mean, y = Mean, ymin = ll, ymax=ul)) +
  geom_point() +
  geom_errorbar(width=0) +
  geom_abline(intercept=0,slope=1,alpha=.5,linetype="dashed") +
  theme_bw() +
  coord_cartesian(xlim=c(-.13,.11), ylim= c(-.13,.11)) +
  theme(aspect.ratio=1)

pred_obs_cor <- rcor(preds_obs$pred_mean, preds_obs$Mean)

print(paste( c("Model x observed correlation, r = ", rcor(preds_obs$pred_mean, preds_obs$Mean))), quote=FALSE)
```

For some reason this simulation is not capturing the changes in vaccTox exactly, which I expected it would. I suppose it could have something to do with the distributions of pre and posttest scores, and/or the long tails in the distribution of change scores. 

In any case, we can still pretty safely take this as an estimate of the correlation between model predictions and observed change scores: r = `r round(pred_obs_cor,3)` and $r^2$ = `r round(pred_obs_cor^2,3)`. That's considerably poorer than the model predictions for disease risk intervention, which correlated approximately r = .85 and got about 75% of the variance. 

#### immune system beliefs?

Looking at that plot, predictions for beliefs about the infant immune system are pretty obviously poor. And, it's easy to imagine that we have inadvertantly targetted those beliefs directly: the intervention describes how the infants' body is able to process naturally-produced formaldehyde, potentially underscoring its robustness. This is my not-quite-right approach to fitting this (should be fitted jointly for both beliefs, can implement that later).


```{r}
vt_iw_model <- add_evid_node(filter_dscored(d_vacctox, "vaccTox"), 
                                  base_model, 
                                  "vaccTox", "_1")

vt_iw_model <- add_evid_node(filter_dscored(d_vacctox, "infantImmWeak"), 
                                  vt_iw_model, 
                                  "infantImmWeak", "_2")

# ----------------------------------------

set.seed(12345)

pred0_vt_iw <- sapply(nodes, function(x){
  statement <- paste0("cpquery(vt_iw_model, event = (", x, " == 'Yes'), evidence=TRUE, n = 1e6)")
  eval(parse(text=statement))
})

pred1_vt_iw <- sapply(nodes, function(x){
  statement <- paste0("cpquery(vt_iw_model, event = (", 
                      x,
                      " == 'Yes'), evidence= (evid_1 == 'Yes' & evid_2== 'Yes'), n = 1e6)")
  eval(parse(text=statement))
})

pred_changes2 <- pred1_vt_iw - pred0_vt_iw

pred_changes2 <- as.data.frame(pred_changes2)

pred_changes2 <- pred_changes2 %>%
  mutate(scale = nodes) %>%
  rename(Mean = pred_changes2) %>%
  mutate(type = "predicted")


preds_obs2 <- s3_scored %>%
  filter(condition == "intervention") %>%
  group_by(scale) %>%
  mutate(
    pre = rescale_beta((pre - 4), -3, 3),
    post = rescale_beta((post - 4), -3, 3)
  ) %>%
  mutate(change = post - pre) %>%
  summarize(
    Mean = mean(change),
    se = sd(change) / sqrt(n())
  ) %>%
  mutate(
    ul = Mean + se,
    ll = Mean - se
  ) %>%
  left_join(pred_changes2 %>% rename(pred_mean = Mean), by = "scale")

preds_obs2 %>%
  ggplot(aes(x=pred_mean, y = Mean, ymin = ll, ymax=ul)) +
  geom_point() +
  geom_errorbar(width=0) +
  geom_abline(intercept=0,slope=1,alpha=.5,linetype="dashed") +
  theme_bw() +
  coord_cartesian(xlim=c(-.13,.11), ylim= c(-.13,.11)) +
  theme(aspect.ratio=1)


print(paste( c("Model x observed correlation, r = ", rcor(preds_obs2$pred_mean, preds_obs2$Mean))), quote=FALSE)
# ----------------------------------------

# obs_pred2 <- bind_rows(pred_changes2, obs_changes) %>%
#   spread(type, Mean) %>%
#   group_by(scale) %>%
#   summarize_all(mean, na.rm=TRUE)
# 
# cor(obs_pred2$predicted, obs_pred2$observed)
# 
# obs_pred2 %>% # a bit of a hack
#   ggplot(aes(x=predicted, y = observed, ymin = ll, ymax=ul)) +
#   geom_abline(slope=1, intercept=0, linetype="dashed", alpha =.5) +
#   geom_point() +
#   geom_errorbar() +
#   coord_cartesian(xlim=c(-.06,.08), ylim=c(-.06,.08)) +
#   theme_bw() +
#   theme(aspect.ratio = 1)
# 
# obs_pred2 %>%
#   gather(type, Mean, observed, predicted) %>%
#   ggplot(aes(y = reorder(scale, Mean), x = Mean, xmin = ll, xmax = ul, color=type)) +
#   geom_point() +
#   geom_errorbarh(height=0)
```

Adding this link improves predictions by cleaning up predictions for immune system weakness (trivially, since we gave it to the model), and immune system limited capacity. Predictions for the remaining beliefs shift around a bit too but with no improvement to fit. Overall, things are still looking a good deal poorer than the model's predictions for the disease risk intervention. It's really considerably worse--whereas for the disease risk intervention the model's predictions were basically numerically right with a few exceptions, here they are numerically wrong with a few exceptions. They capture the overall trend, but don't really seem to capture all the systematic changes. The magnitudes are also off, with the model generally predicting larger changes than were observed. Not to mention, the failure to show anything clear with intentions really undercuts the applied perspective. 

## Conclusions

As far as I can tell, this is a somewhat disappointing result for the study. We might ...

* try again/get new data?
* get more data? (Could do [sequential analysis](https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2023), though I think adding to the sample until the bias goes away would require a lot of data ...)

----------------------------------------

### Some miscellany

change scores: plot and effect size

Change score for vaccTox is huge, d = .81--even bigger than the pilot study! but part of that is driven by a small (d=.11) increase in the control condition that is nearly significant by traditional standards. So, maybe the real increase is still comparable to what we originally observed.

```{r}

s3_scored %>%
  filter(scale=="vaccTox") %>%
  ggplot(aes(x=condition, y = change)) +
  geom_boxplot(width=.25) +
  theme_bw()

s3_scored %>%
  filter(scale=="vaccTox") %>%
  ggplot(aes(x=condition, y = change)) +
  stat_summary(fun.y = mean, geom = "point", size=3) + 
  stat_summary(fun.data = mean_se, geom = "errorbar", width=.1) +
  theme_bw() +
  labs(x="Condition", y = "Mean Change Score")
```


I can quickly hack together the predictions we mdae in the preregistration with the observed changes. The absolute values will be off (way off b/c of rescaling) but the correlations would still be indicative. looks like there's a substantive correlation but not as strong as prior intervention study, also clearly biased, maybe a result of differences between pilot and full study and some scale bound issues?

```{r}
preds <-  read_csv("../data_private/model_preds_pilot.csv") %>%
  rename(pred_mean = Mean)

preds_obs <- s3_scored %>%
    group_by(condition, scale) %>%
  summarize(
    Mean = mean(change),
    se = sd(change)/sqrt(n())
    ) %>%
  mutate(
    ul = Mean + se,
    ll = Mean - se
  ) %>%
  filter(condition=="intervention") %>%
  left_join(preds, by ="scale")

cors <- s3_scored %>%
  ungroup() %>%
  select(workerId, scale, pre) %>%
  spread(scale, pre) %>%
  select(-workerId) %>%
  cor() %>%
  .["vaccTox",] %>%
  tibble(pred_cor = ., scale=names(.))

preds_obs <- left_join(preds_obs, cors, by="scale")

preds_obs %>%
  ggplot(aes(x=pred_mean, y = Mean, ymin = ll, ymax=ul)) +
  geom_point() +
  geom_errorbar(width=0) +
  theme_bw() +
  theme(aspect.ratio=1)

print(paste( c("Model x observed correlation, r = ", rcor(-1*preds_obs$pred_cor, preds_obs$Mean))), quote=FALSE)
```

What if we computed changes as difference between intervention and no intervention conditions? Answer: it looks pretty similar. And, no good reason really to support doing it this way over the other way.

```{r}
preds_obs2 <- s3_scored %>%
    group_by(condition, scale) %>%
  summarize(
    Mean = mean(change)
    # se = sd(change)/sqrt(n())
    ) %>%
  spread(condition, Mean) %>%
  mutate(difference = intervention - noInterv) %>%
  left_join(cors, by ="scale") %>%
  mutate(pred_cor = pred_cor/sd(.$pred_cor)*-1,
         difference = difference/sd(.$difference),
         intervention = intervention/sd(.$intervention)
         )

preds_obs2 %>%
  ggplot(aes(x=pred_cor, y=difference)) +
  geom_point() +
  geom_text(aes(label=scale)) +
  # geom_abline(intercept=0, slope=1) +
  # geom_errorbar(width=0) +
  theme_bw() +
  theme(aspect.ratio=1)


print(paste( c("Model x observed correlation, r = ", rcor(preds_obs2$difference, preds_obs2$pred_cor))), quote=FALSE)
```


### Participant locations ...

Looks like our participants come from all over the U.S., as I had hoped based on when I posted the study.


```{r}
# install.packages("maps")

loc_data <- post %>%
  filter(Progress==100, check_3==3, check_5==5) %>%
  select(workerId, LocationLatitude, LocationLongitude, condition) %>%
  filter(LocationLatitude < 50, LocationLatitude > 24, LocationLongitude < -50)

usa <- map_data("state") # we already did this, but we can do it again

ggplot() +
  geom_polygon(
    data = usa,
    aes(x = long, y = lat, fill = region, group = group),
    color = "white"
  ) +
  geom_point(
    data = loc_data,
    aes(x = LocationLongitude, y = LocationLatitude, color = condition),
    shape = 16, alpha = .75
  ) +
  # geom_hex(data = loc_data,
  #                 aes(x = LocationLongitude, y = LocationLatitude), binwidth=2) +
  # scale_fill_viridis_c() +
  coord_fixed(1.3) +
  theme_minimal() +
  scale_fill_grey() +
  guides(fill = FALSE) # do this to leave off the color legend
```

## Modeling effects in control condition

Can we model the effects in the control condition if we imagine that something directly affected participants' vaccine intentions?


```{r}
## WARNING: THIS CODE IS HACKY!
## 
## script for generating model predictions based on vaccine toxins intervention
## run:
## int-theory-vacc/notebooks/s2-modeling.Rmd (to line 495)
##
## then run this notebook, and finally this code:

filter_dscored <- function(data, scale_name){
  
  data %>%
  filter(scale == scale_name) %>%
  mutate(mean = rescale_beta(mean, -3, 3)) %>%
  spread(phase, mean) %>% 
  mutate(condition = relevel(condition, ref="noInterv")) %>%
  mutate(evid = 1)
}

# # update model fitting thing ignore control condition b/c of strange things in this data ...

find_evid_ratio <- function(data){
    optim(par = c(0, 5), fn = evid_score_func, data = data, method="BFGS")
}

# find_evid_ratio <- function(data){
#   # HACK HACK HACK
#   return (data.frame(par = c(-.55, 7)))
# }

evid_score_func <- function(data, par) {
    
    inv_logit <- plogis
    logit <- qlogis
    
    pred_y <- with(data,
                   {
                       inv_logit( logit(pre) + evid*par[1] )
                   }
    )
    pred_beta_A <- pred_y * par[2]
    pred_beta_B <- (1-pred_y) * par[2]
    
    ll <- -1 * sum(dbeta(data$post, pred_beta_A, pred_beta_B, log=TRUE)) # beta regression
    return(ll)
}

get_evid_probs <- function(result){
  # this is a heuristic/hack, could be made better but probably doesn't matter
  evid_ratio <- exp(result$par[1])
  if (evid_ratio < 1) {
    p0 <- .90
  } else {
    p0 <- .5
  }
  
  if (p0*evid_ratio >= 1) {
    p0 <- p0*.9/(p0*evid_ratio)
  }

  p1 <- p0*evid_ratio
  
  c(p0,p1)
}

predModel <- add_evid_node(filter_dscored(d_vacctox, "vaccIntent"), base_model, "vaccIntent")

pred0 <- sapply(nodes, function(x){
  statement <- paste0("cpquery(predModel, event = (", x, " == 'Yes'), evidence=TRUE, n = 5e5)")
  eval(parse(text=statement))
})

pred1 <- sapply(nodes, function(x){
  statement <- paste0("cpquery(predModel, event = (", x, " == 'Yes'), evidence= (evid == 'Yes'), n = 5e5)")
  eval(parse(text=statement))
})

pred_changes <- pred1 - pred0

pred_changes <- as.data.frame(pred_changes)

pred_changes <- pred_changes %>%
  mutate(scale = nodes) %>%
  rename(Mean = pred_changes) %>%
  mutate(type = "predicted")

preds_obs <- s3_scored %>%
  filter(condition == "noInterv") %>%
  group_by(scale) %>%
  mutate(
    pre = rescale_beta((pre - 4), -3, 3),
    post = rescale_beta((post - 4), -3, 3)
  ) %>%
  mutate(change = post - pre) %>%
  summarize(
    Mean = mean(change),
    se = sd(change) / sqrt(n())
  ) %>%
  mutate(
    ul = Mean + se,
    ll = Mean - se
  ) %>%
  left_join(pred_changes %>% rename(pred_mean = Mean), by = "scale")

preds_obs %>%
  rename(
    observed = Mean,
    predicted = pred_mean
  ) %>%
  gather(type, Mean, observed, predicted) %>%
  ggplot(aes(y = reorder(scale, Mean), x = Mean, xmin = ll, xmax = ul, color=type)) +
  geom_point()

preds_obs %>%
  ggplot(aes(x=pred_mean, y = Mean, ymin = ll, ymax=ul)) +
  geom_point() +
  geom_errorbar(width=0) +
  geom_abline(intercept=0,slope=1,alpha=.5,linetype="dashed") +
  theme_bw() +
  coord_cartesian(xlim=c(-.04,.04), ylim= c(-.04,.04)) +
  theme(aspect.ratio=1)

pred_obs_cor <- rcor(preds_obs$pred_mean, preds_obs$Mean)

print(paste( c("Model x observed correlation, r = ", rcor(preds_obs$pred_mean, preds_obs$Mean))), quote=FALSE)
```

That's actually a much better fit than I expected! But it's all driven by the shift on vaccine intentions, not likely to be meaningful among hte 13 remaining variables.

```{r}
z <-  preds_obs %>% filter(scale!="vaccIntent")
rcor(z$pred_mean, z$Mean)
```


## Is vaccIntent less correlated with other beliefs at pre/post in interv/control conditions (this is a variant of ellen's question about the model's predictions that will be much faster to answer)

```{r}
pre_control <- summary(lm(
  vaccIntent ~ vaccDanger + vaccEff + vaccStrain + vaccTox + diseaseRare + diseaseSevere + diseaseRare + medSkept + hb + nat + infantImmLimCap + infantImmWeak,
  data = s3_scored %>% 
    gather(phase, mean, pre, post) %>% 
    select(-parent, -change) %>%
    spread(scale, mean) %>% 
    filter(phase=="pre", condition=="noInterv")
))

post_control <- summary(lm(
  vaccIntent ~ vaccDanger + vaccEff + vaccStrain + vaccTox + diseaseRare + diseaseSevere + diseaseRare + medSkept + hb + nat + infantImmLimCap + infantImmWeak,
  data = s3_scored %>% 
    gather(phase, mean, pre, post) %>% 
    select(-parent, -change) %>%
    spread(scale, mean) %>% 
    filter(phase=="post", condition=="noInterv")
))

pre_interv <- summary(lm(
  vaccIntent ~ vaccDanger + vaccEff + vaccStrain + vaccTox + diseaseRare + diseaseSevere + diseaseRare + medSkept + hb + nat + infantImmLimCap + infantImmWeak,
  data = s3_scored %>% 
    gather(phase, mean, pre, post) %>% 
    select(-parent, -change) %>%
    spread(scale, mean) %>% 
    filter(phase=="pre", condition=="intervention")
))

post_interv <- summary(lm(
  vaccIntent ~ vaccDanger + vaccEff + vaccStrain + vaccTox + diseaseRare + diseaseSevere + diseaseRare + medSkept + hb + nat + infantImmLimCap + infantImmWeak,
  data = s3_scored %>% 
    gather(phase, mean, pre, post) %>% 
    select(-parent, -change) %>%
    spread(scale, mean) %>% 
    filter(phase=="post", condition=="intervention")
))

pre_control$r.squared
pre_interv$r.squared
post_control$r.squared
post_interv$r.squared

```

So vaccIntent is slightly more predictable (more tightly coupled with other beliefs) in the control condition at pretest than in other measures, by a fairly sizable amount, 4-9% of the variance (not sure how robust that is though).
