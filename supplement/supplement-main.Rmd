---
title: "Supplement"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r s1_load}
source("../code/load-data-s1.R", chdir = TRUE)

library(tidyverse)
library(bnlearn)
library(HydeNet)

source("../code/custom-structure-learning/cog-model-main.R", chdir = TRUE)
source("../code/graph-model-tools.R")
source("../code/custom-structure-learning/cog-model-jags-tools.R")

d_bn_scaled <- d_bn %>%
 mutate_all(function(x){rescale_beta(x,-3,3)})

## set the seed to make your partition reproductible
set.seed(123)
trainInd <- sample(seq_len(nrow(d_bn_scaled)), size = floor(nrow(d_bn_scaled)*.80))

train <- d_bn_scaled[trainInd, ]
test <- d_bn_scaled[-trainInd, ]
```

```{r cross_validation}
# source("../code/custom-structure-learning/fit-cog-models-bnlearn.R")
cv_res <- readRDS("../local/cv-res.rds")

cv_res %>% 
  ggplot(aes(x=reorder(algo, loss), y=loss)) +
  geom_boxplot() +
  coord_flip()

# cv_res %>% 
#   group_by(algo) %>% 
#   summarize(M = mean(loss), se = sd(loss)) %>% 
#   ggplot(aes(x=algo, y=M, ymin=M-se, ymax=M+se)) +
#   geom_pointrange() +
#   coord_flip()
```
All algorithms produce fits of similar quality. Data doesn't tell which to pick from among these. So it's basically up to us to choose whichever result makes most sense or we like for whatever reason. And to just caveat that it's not the "one true model." 

```{r fit_all_models}

hc_args <- list(score="custom", fun=cog_model_score_func, maxp=5, max.iter=1000)

models <- list(
  hc = partial(hc, score="custom", fun=cog_model_score_func, maxp=5, max.iter=1000),
  hc_intentdv = partial(hc, blacklist=intent_dv_bl, score="custom", fun=cog_model_score_func, maxp=5, max.iter=1000),
  hc_theory = partial(hc, blacklist=theory_bl, score="custom", fun=cog_model_score_func, maxp=5, max.iter=1000),
  mmhc_05 = partial(mmhc, restrict.args=list(alpha=.05), maximize.args = hc_args),
  mmhc_01 = partial(mmhc, restrict.args=list(alpha=.01), maximize.args = hc_args),
  mmhc_intentdv_05 = partial(mmhc, blacklist = intent_dv_bl, restrict.args=list(alpha=.05), maximize.args = hc_args),
  mmhc_intentdv_01 = partial(mmhc, blacklist = intent_dv_bl, restrict.args=list(alpha=.01), maximize.args = hc_args),
  mmhc_theory_05 = partial(mmhc, blacklist = theory_bl, restrict.args=list(alpha=.05), maximize.args = hc_args),
  mmhc_theory_01 = partial(mmhc, blacklist = theory_bl, restrict.args=list(alpha=.01), maximize.args = hc_args)
)

models <- mclapply(
    models, 
    function(f){f(train)}, 
    mc.cores = detectCores(logical=TRUE)
  )

arc_counts <- map(models, ~nrow(arcs(.x)))
arc_counts
```

```{r}
# this is the favorite son (most likely)
h <- HydeNetwork(as.formula(bnlearn_to_hyde_string(models$hc_theory)))
plot(h)
```
HC theory looks nice and interpretable, tho there are a fair number of edges. 

We should maybe think on our theory-based hierarchy structure a bit more too. Where does overparenting belong? Maybe the abstract as parents intent dv is the way to go? 

```{r}
# # can bootstrap strength of arcs for winning model approach, focus interpretations on strength of arcs / parameterizations

## very slow!
# boot.strength(train, R=200, algorithm="hc", algorithm.args = list(score="custom", fun=cog_model_score_func))
```


```{r}

predict_node <- function(targVar, model_string, data){
  
  tempDF <- data
  tempDF[[targVar]] <- NA
  tempDF[[targVar]] <- as.numeric(tempDF[[targVar]])
  imputed <- make_predictions(model_string, data, nodes_to_predict = targVar, iter=1e4)
  return(imputed[[targVar]])
}


main_model <- models$hc_theory
model_string <- suppressWarnings(write_jags_model(main_model, train))

# takes ~3m on 8 cores -------
start_time <- Sys.time()

predictions <- test %>% mutate_all(function(x){NA}) %>% mutate_all(as.numeric)

x <- mclapply(
  colnames(test),
  predict_node,
  model_string = model_string,
  data = test,
  mc.cores=8
)

names(x) <- colnames(test)

print(Sys.time() - start_time)
```

Inspect absolute fit

```{r validation_plot fig.height=6, fig.width=10}
predictions <- as_tibble(x) %>% mutate(subj = 1:n()) %>% gather(varName, predValue, -subj) 
test_analyze <- test %>%
  gather(varName, value) %>% 
  mutate(predValue = predictions$predValue)

recode_nodes <- function(x){
  recode(x,
         "diseaseRare" = "Disease Rarity",
         "diseaseSevere" = "Disease Severity",
         "hb" = "Holistic Balance",
         "infantImmLimCap" = "IIS: Limited\nCapacity",
         "infantImmWeak" = "IIS: Weakness",
         "medSkept" = "Medical\nSkepticism",
         "nat" = "Naturalism",
         "overpar" = "Parental\nProtectiveness",
         "parentExpert" = "Parental\nExpertise",
         "vaccDanger" = "Vaccine\nDanger",
         "vaccEff" = "Vaccine\nEffectiveness",
         "vaccIntent" = "Vaccination\nIntentions",
         "vaccStrain" = "IIS: Vaccines\nStrain",
         "vaccTox" = "Vaccines\nToxicity")}

predObsCor <- cor(test_analyze$value,test_analyze$predValue)

corrDF <- test_analyze %>%
  mutate(varName = recode_nodes(varName)) %>%
  group_by(varName) %>%
  summarize(cor = cor(value,predValue)) %>%
  mutate(cor = format(round(cor,2),nsmall=2))

plt.valid <- test_analyze %>%
  mutate(varName = recode_nodes(varName)) %>%
  ggplot(aes(x=predValue, y=value)) + 
  geom_abline(slope=1, intercept=0, linetype="dashed", alpha=.33) +
  geom_point(shape=1, size=.5, color="darkturquoise") + 
  geom_text(data=corrDF, aes(label=paste("r =", cor)),
            x=-Inf, y=Inf, hjust=-0.12, vjust=1.4,
            size = 10*.352777778,
            color="grey25") +
  # geom_smooth(method="lm") +
  coord_fixed() + 
  facet_wrap(~varName, nrow=2) + 
  scale_x_continuous("Predicted values", limits = c(0, 1)) +
  scale_y_continuous("Observed values", limits = c(0, 1)) +
  theme_bw(base_size = 10) +
  theme(strip.background = element_rect(fill="grey90"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

plt.valid
# ggsave(plot = plt.valid, filename = "../local/s1_predobs_plot.png", width=8.5, height=4)
```

Maybe compare predicted correlation matrix against true correlation matrix? Could bootstrap resample and create distributon of distances to represent null, see where this falls? Hope would be for non-sig result, that the reconstructed corr matrix is within resampling error. Maybe not necessary to do this tho?

# Study 1

NOTE: these code blocks are cut/pasted from other nb and not yet tested.

```{r s1}
# data is already loaded
source("../code/load-data-s1.R")

```

A total of `r nrow(s1_all)` people recruited from Amazon Mechanical Turk (mTurk) participated in this study conducted in August of 2017. Participants were paid $1.60 for about 8 minutes of their time. `r s1_n_excluded` participants (6%) failed at least one of two attention check questions and were excluded from further analysis, leaving a final sample of n = `nrow(d_demo)`. After completing all 14 scales, participants were asked standard demographics questions as well as a handful of questions about whether they were or were expecting to be a parent. They were also debriefed with a brief statement noting that vaccines are safe and pointing them toward a WHO website for further information. 

```{r}

compute_alpha <- function(data, varName){
  data %>%
  filter(question_block == varName) %>%
  select(workerId, question, response_num_rev) %>%
  spread(question, response_num_rev) %>%
  remove_rownames() %>%
  column_to_rownames("workerId") %>%
  psych::alpha()
}

# scale reliability analyses

var_names <- as.character(unique(s1_long$question_block))
alpha_values <- map(var_names, ~compute_alpha(s1_long,.x)$total[[2]])
names(alpha_values) <- var_names

# do something to pretty this up

as_tibble(alpha_values) %>% 
  gather(scale, alpha)

```

# Study 2

```{r s2}
# load data
suppressWarnings(suppressMessages(source("../code/load-data-s2.R", chdir=TRUE)))
```

_Participants._ We recruited `r s2_meta$n_recruited` people to participate in a two-day study via mTurk in April 2018. Two days later, participants who completed part 1 and passed attention checks were invited via email to participate in Day 2 of the study, with `r s2_meta$n_recruited_post` participants returning (`r s2_meta$n_eligible` invited, retaining `r round(s2_meta$n_recruited_post/s2_meta$n_eligible*100,1)`% of the Day 1 sample). Participants were paid \$1.35 for their participation in each phase of the study. `r s2_meta$n_failed_post` participants (`r round( (s2_meta$n_failed_post/s2_meta$n_recruited_post)*100,1)`%) failed at least one attention check on Day 2 and were excluded from further analysis. We also excluded from analyses `r s2_meta$n_duplicate` additional participants who accessed the second survey more than once. This left a final sample of `r s2_meta$n_complete` participants.


```{r}

recode_scales <- function(x){
  recode(x,
         "diseaseRare" = "Disease Rarity",
         "diseaseSevere" = "Disease Severity",
         "hb" = "Holistic Balance",
         "infantImmLimCap" = "IIS: Lim. Capacity",
         "infantImmWeak" = "IIS: Weakness",
         "medSkept" = "Med. Skepticism",
         "nat" = "Naturalism",
         "overpar" = "Parental Protectiveness",
         "parentExpert" = "Parental Expertise",
         "vaccDanger" = "Vaccine Danger",
         "vaccEff" = "Vaccine Eff.",
         "vaccIntent" = "Vaccination Intentions",
         "vaccStrain" = "IIS: Vaccines Strain",
         "vaccTox" = "Vaccines Toxicity")}

# create plot of differences for all 14 scales

s2_scored %>% 
  spread(phase, mean) %>% 
  drop_na(scale) %>% 
  mutate(diff = post-pre) %>% 
  mutate(
    scale = recode_scales(scale),
    condition = ifelse(condition=="noInterv", "Control", "Disease Risk")
    ) %>% 
  group_by(condition, scale) %>%
  summarize(M = mean(diff, na.rm=TRUE), se = sd(diff)/sqrt(n())) %>% 
  ggplot(aes(x = reorder(scale,M), y = M, ymin = M-se*2, ymax=M+se*2, color=condition)) +
  geom_pointrange(position=position_dodge(width=.25)) +
  geom_hline(yintercept=0, linetype="dashed", color="grey") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    panel.grid = element_blank()
    ) +
  labs(title="Study 2", y = "Mean difference", x = "Belief scale", color="Intervention")
```
```{r}
# compare conditions on vaccIntent scale w/ regression model
# probably do ordinal model with brms

# fit evidence node

# generate model predictions + create plot
```


# Study 3a and 3b

```{r s3}
# load data
source("../code/load-data-s3.R")
```

_Participants._ We recruited `r s3_meta$n_recruited` people to participate in a two-day study via mTurk in April (Experiment 3A) and June (Experiment 3B) of 2018. One day after their initial participation, participants who completed part 1 and passed attention checks were invited via email to participate in Day 2 of the study, with `r s3_meta$n_recruited_post` participants returning (`r s3_meta$n_eligible` invited, retaining `r round(s3_meta$n_recruited_post/s3_meta$n_eligible*100,1)`% of the Day 1 sample). Participants were paid \$1.35 for their participation in each phase of the study and were allowed to participate in part 2 up to two days after their initial participation in part 1. `r s3_meta$n_failed_post` participants (`r round( (s3_meta$n_failed_post/s3_meta$n_recruited_post)*100,1)`%) failed at least one attention check on Day 2 and were excluded from further analysis.  This left a final sample of `r s3_meta$n_complete` participants.

```{r}

# compare conditions on vaccIntent w/ regression model

# create plot of differences for all 14 scales

s3_scored %>% 
  # spread(phase, mean) %>% 
  drop_na(scale) %>% 
  mutate(diff = post-pre) %>% 
  mutate(
    scale = recode_scales(scale),
    condition = ifelse(condition=="noInterv", "Control", "Disease Risk")
    ) %>% 
  group_by(condition, scale) %>%
  summarize(M = mean(diff, na.rm=TRUE), se = sd(diff)/sqrt(n())) %>% 
  ggplot(aes(x = reorder(scale,M), y = M, ymin = M-se*2, ymax=M+se*2, color=condition)) +
  geom_pointrange(position=position_dodge(width=.25)) +
  geom_hline(yintercept=0, linetype="dashed", color="grey") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    panel.grid = element_blank()
    ) +
  labs(title="Study 3a and 3b", y = "Mean difference", x = "Belief scale", color="Intervention")

# fit evidence node

# generate model predictions + create plot
```

# Study 4

```{r}
# load data
source("../code/load-data-s4.R")

s4_meta <- list(
  n_recruited = s4_wide %>% filter(workerId %in% s1_all$workerId) %>%  distinct(workerId) %>% nrow(),
  n_recruited_post = s4_wide %>% filter(workerId %in% s1_all$workerId) %>%  distinct(workerId) %>% nrow(),
  n_failed_post = s4_wide %>% 
    filter(check_5!=5 | check_3!=3) %>% 
    filter(workerId %in% s1_all$workerId) %>% distinct(workerId) %>% nrow(), #%>% ,
  n_complete =s4_wide %>% 
    filter(check_5==5, check_3==3, workerId %in% s1_all$workerId) %>% distinct(workerId) %>% nrow()
)
```

We attempted to contact and re-recruit as many of the `r nrow(d_bn)` participants who completed Study 1 to participate in a follow-up study in May of 2019. We were able to re-recruit `r s4_meta$n_recruited` participants and of these`r n_complete` successfully completed the study including passing all attention checks.

```{r}
s1 <- d_sum %>%
  rename(scale = question_block) %>%
  group_by(scale, workerId) %>% 
  summarize(s1 = mean(score, na.rm = T)) %>%
  ungroup()

combined <- s4_long  %>%
  # filter(check_5==5, check_3==3, Progress==100) %>% 
  filter(workerId %in% s1$workerId) %>%
  # select(workerId, scale, item, resp, vacc_news) %>%
  group_by(workerId, scale, vacc_news) %>%
  summarize(posttest = mean(resp)-4) %>%
  ungroup() %>% 
  right_join(s1) %>%
  rename(pretest = s1) %>%
  mutate(returned = !is.na(posttest))
```


```{r s4}
# compare effects in graph

combined %>% 
  filter(returned==TRUE, !is.na(vacc_news)) %>% # somehow 5? participants slippd by without answering 
  mutate(diff = posttest-pretest) %>%
  mutate(
    vacc_news = as.factor(vacc_news),
    scale = recode_scales(scale)
    ) %>%
  group_by(vacc_news,scale) %>%
  summarize(
    mean = mean(diff),
    se = sd(diff)/sqrt(n()),
    ul = mean + se,
    ll = mean - se,
    ul95 = mean + 1.96*se,
    ll95 = mean - 1.96*se
  ) %>%
  ungroup() %>%
  mutate(vacc_news = factor(vacc_news, c(1,0), labels=c("Yes","No"))) %>%
  # mutate(phase = ordered(phase, levels=c("pretest","posttest"))) %>%
  ggplot(aes(x=reorder(scale, mean), y = mean, ymin =ll, ymax=ul, color=vacc_news)) +
  geom_point(position=position_dodge(width=.25), size=2, shape=16) +
  geom_errorbar(position=position_dodge(width=.25), size=.75, width=0, alpha=.8) +
  geom_errorbar(aes(ymin=ll95, ymax = ul95), width=0, position=position_dodge(width=.25), alpha=.5) +
  geom_hline(yintercept = 0, linetype="dashed",alpha=.5) +
  # facet_wrap(~scale, scales="free") +
  theme_bw(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = "right",
    panel.grid = element_blank()) +
  labs(title="Study 4", y = "Change scores", x = "Belief scale", color ="News\nexposure")

# compare conditions on vaccIntent w/ regression model

# fit evidence node

# generate model predictions + create plot
```

# Overall demographics

```{r}
# compute overall demographics for all studies for paper

# and create big table with table1 and t1kable()
```

